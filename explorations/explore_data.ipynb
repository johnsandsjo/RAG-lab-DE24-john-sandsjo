{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572afbab",
   "metadata": {},
   "source": [
    "# Explore the data in vectore db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77cba88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceDBConnection(uri='/Users/john.sandsjo/Documents/github/RAG-lab-DE24-john-sandsjo/transcript_repo')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "from pathlib import Path\n",
    "\n",
    "VECTOR_DB_PATH = Path.cwd().parent / \"transcript_repo\"\n",
    "vector_db = lancedb.connect(uri= VECTOR_DB_PATH)\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4c3ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transcriptions']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16f3df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceTable(name='transcriptions', version=61, _conn=LanceDBConnection(uri='/Users/john.sandsjo/Documents/github/RAG-lab-DE24-john-sandsjo/transcript_repo'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"transcriptions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff325473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fastapi crud app</td>\n",
       "      <td># Fastapi CRUD app\\n\\n**Kokchun Giang:** [00:0...</td>\n",
       "      <td>[-0.012556567, -0.01593715, 0.022671303, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sql analytics course with duckdb - set theory ...</td>\n",
       "      <td># SQL analytics course with DuckDB - set theor...</td>\n",
       "      <td>[-0.027204884, -0.023552606, 0.019531347, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python fundamentals</td>\n",
       "      <td># Python fundamentals\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.011976539, 0.0025688808, 0.015519834, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>packaging in python</td>\n",
       "      <td># Packaging in python\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.0150246415, 0.0042777252, 0.038052212, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pydantic fundamentals</td>\n",
       "      <td># Pydantic fundamentals\\n\\n[00:00:00] Hello an...</td>\n",
       "      <td>[-0.017777685, -0.01204192, 0.02006065, -0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sql analytics course with duckdb - joins with ...</td>\n",
       "      <td># SQL analytics course with DuckDB - joins wit...</td>\n",
       "      <td>[-0.022949534, -0.022863599, 0.013395227, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pydantic with gemini to structure output in a ...</td>\n",
       "      <td># Pydantic with gemini to structure output in ...</td>\n",
       "      <td>[-0.01512796, -0.003907627, 0.015176425, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pydanticai chatbot</td>\n",
       "      <td># pydanticAI chatbot\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.012143856, -0.001205422, 0.013563879, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sql analytics course with duckdb - joins concepts</td>\n",
       "      <td># SQL analytics course with DuckDB - joins con...</td>\n",
       "      <td>[-0.027722066, -0.016966412, 0.012043696, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>docker setup windows</td>\n",
       "      <td># docker setup windows\\n\\n[00:00:00] Hello and...</td>\n",
       "      <td>[-0.0034482135, 0.010729573, 0.016939094, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sql analytics course with duckdb - sakila bi d...</td>\n",
       "      <td># SQL analytics course with DuckDB - Sakila BI...</td>\n",
       "      <td>[0.0055291746, -0.004573792, 0.019058505, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sql analytics course with duckdb - setup duckdb</td>\n",
       "      <td># SQL analytics course with DuckDB - setup duc...</td>\n",
       "      <td>[-0.008761382, -0.0034869136, 0.01760935, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sql analytics course with duckdb - strings con...</td>\n",
       "      <td># SQL analytics course with DuckDB - strings c...</td>\n",
       "      <td>[-0.02525857, -0.011639317, 0.004399808, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logistic regression theory</td>\n",
       "      <td># Logistic regression theory\\n\\n[00:00:00] Hel...</td>\n",
       "      <td>[-0.008146983, -0.00016716555, 0.011266027, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>api trafiklab (1)</td>\n",
       "      <td># API trafiklab\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[-0.006812348, -0.0044766036, 0.014319714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sql analytics course with duckdb - pandas and ...</td>\n",
       "      <td># SQL analytics course with DuckDB - pandas an...</td>\n",
       "      <td>[-0.009578243, -0.011996515, 0.012965736, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sql analytics course with duckdb - views tutorial</td>\n",
       "      <td># SQL analytics course with DuckDB - views tut...</td>\n",
       "      <td>[-0.027096331, -0.0073001576, 0.0048975083, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pandas_read_excel</td>\n",
       "      <td># pandas\\_read\\_excel\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.0038475876, 0.0054737586, 0.024301918, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>python intro</td>\n",
       "      <td># python intro\\n\\n[00:00:00] Hello and welcome...</td>\n",
       "      <td>[-0.0110368235, -0.0020397531, 0.012955041, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pytest unit testing</td>\n",
       "      <td># pytest unit testing\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.014343338, 0.0007622975, 0.030120881, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sql analytics course with duckdb - sakila bi d...</td>\n",
       "      <td># SQL analytics course with DuckDB - Sakila BI...</td>\n",
       "      <td>[0.0055291746, -0.004573792, 0.019058505, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data platform course structure</td>\n",
       "      <td># Data platform course structure\\n\\n[00:00:00]...</td>\n",
       "      <td>[0.010435624, -0.0016020014, 0.011163727, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>course structure for azure two weeks course</td>\n",
       "      <td># Course structure for Azure two weeks course\\...</td>\n",
       "      <td>[0.0015313567, 0.012794174, 0.016358217, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>modern data stack - dockerize your data pipeline</td>\n",
       "      <td># Modern data stack - dockerize your data pipe...</td>\n",
       "      <td>[0.0027987233, 0.014228618, 0.027441906, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>xgboost hands on tutorial for classification</td>\n",
       "      <td># XGBoost hands on tutorial for classification...</td>\n",
       "      <td>[-0.004887973, 0.008207077, 0.013008361, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>data storytelling</td>\n",
       "      <td># data storytelling\\n\\n[00:00:00] Hello and we...</td>\n",
       "      <td>[-0.011316549, 0.016874935, 0.012392512, -0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>api trafiklab</td>\n",
       "      <td># API trafiklab\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[-0.006812348, -0.0044766036, 0.014319714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sql analytics course with duckdb - course stru...</td>\n",
       "      <td># SQL analytics course with DuckDB - course st...</td>\n",
       "      <td>[0.0023245383, -0.0029253534, 0.0036489798, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fastapi and scikit-learn api connect to stream...</td>\n",
       "      <td># FastAPI and scikit-learn API connect to stre...</td>\n",
       "      <td>[-0.017560659, 0.009985835, 0.01749353, -0.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sql analytics course with duckdb - crud operat...</td>\n",
       "      <td># SQL analytics course with DuckDB - CRUD oper...</td>\n",
       "      <td>[-0.018357037, -0.0009783215, 0.025988214, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sql analytics course with duckdb - pandas and ...</td>\n",
       "      <td># SQL analytics course with DuckDB - pandas an...</td>\n",
       "      <td>[-0.009578243, -0.011996515, 0.012965736, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pydanticai fundamentals - outputting structure...</td>\n",
       "      <td># PydanticAI fundamentals - outputting structu...</td>\n",
       "      <td>[-0.033087507, 0.0036856267, 0.02514256, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>chat with your excel data - xlwings lite (1)</td>\n",
       "      <td># Chat with your excel data - xlwings lite\\n\\n...</td>\n",
       "      <td>[-0.0071265996, 0.020476552, 0.017235534, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>dbt modeling snowflake</td>\n",
       "      <td># dbt modeling snowflake\\n\\n**Kokchun Giang:**...</td>\n",
       "      <td>[-0.010636117, 0.01212832, 0.029030465, -0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>serving pydanticai gemini model with fastapi t...</td>\n",
       "      <td># Serving PydanticAI Gemini Model with FastAPI...</td>\n",
       "      <td>[-0.018388461, -0.006911759, 0.015190295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>postgres sink</td>\n",
       "      <td># postgres sink\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[0.007228276, 0.021961471, 0.031665433, -0.088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>python_oop_1</td>\n",
       "      <td># Python\\_oop\\_1\\n\\n**kokchun:** [00:00:00] He...</td>\n",
       "      <td>[-0.008043373, -0.011165032, 0.031197485, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>chat with your excel data - xlwings lite</td>\n",
       "      <td># Chat with your excel data - xlwings lite\\n\\n...</td>\n",
       "      <td>[-0.0071265996, 0.020476552, 0.017235534, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>an introduction to the vector database lancedb</td>\n",
       "      <td># An introduction to the vector database Lance...</td>\n",
       "      <td>[-0.038686633, 0.0036908067, 0.02178414, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>serving pydanticai gemini model with fastapi t...</td>\n",
       "      <td># Serving PydanticAI Gemini Model with FastAPI...</td>\n",
       "      <td>[-0.018388461, -0.006911759, 0.015190295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>terraform setup</td>\n",
       "      <td># Terraform setup\\n\\n[00:00:00] Hello and welc...</td>\n",
       "      <td>[-0.004057311, 0.01868715, 0.01563467, -0.0808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sql analytics with duckdb - introduction</td>\n",
       "      <td># SQL analytics with DuckDB - introduction\\n\\n...</td>\n",
       "      <td>[-0.028552804, -0.0118231885, 0.00639494, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sql analytics course with duckdb - set theory ...</td>\n",
       "      <td># SQL analytics course with DuckDB - set theor...</td>\n",
       "      <td>[-0.027204884, -0.023552606, 0.019531347, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>data processing course  structure</td>\n",
       "      <td># data processing course structure\\n\\n**Kokchu...</td>\n",
       "      <td>[0.0032914313, 0.01059015, 0.009196502, -0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>modern data stack - deploy dockerized dashboar...</td>\n",
       "      <td># Modern data stack - deploy dockerized dashbo...</td>\n",
       "      <td>[-0.0075815883, 0.02116889, 0.027429571, -0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sql analytics course with duckdb - subquery tu...</td>\n",
       "      <td># SQL analytics course with DuckDB - subquery ...</td>\n",
       "      <td>[-0.012517108, -0.014230472, -0.0019505646, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>logistic regression hands on with scikit learn</td>\n",
       "      <td># Logistic regression hands on with scikit lea...</td>\n",
       "      <td>[-0.0066825696, 0.002244388, 0.011106265, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sql analytics course with duckdb - dlt to load...</td>\n",
       "      <td># SQL analytics course with DuckDB - dlt to lo...</td>\n",
       "      <td>[-0.0066205882, -0.0075128363, 0.019882608, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>modern data stack - using dlt to extract and l...</td>\n",
       "      <td># Modern data stack - using dlt to extract and...</td>\n",
       "      <td>[-0.00826649, 0.019363934, 0.016988434, -0.080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>hands on regularization</td>\n",
       "      <td># Hands on regularization\\n\\n[00:00:00] Hello ...</td>\n",
       "      <td>[-0.009709013, 6.357031e-05, 0.023912106, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>azure static web app deploy react app</td>\n",
       "      <td># Azure static web app deploy react app\\n\\n[00...</td>\n",
       "      <td>[-0.012295628, 0.015832268, 0.025340993, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>how does llm work_</td>\n",
       "      <td># How does LLM work?\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.027908303, 0.017137818, 0.023856219, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>sql analytics course with duckdb - strings tut...</td>\n",
       "      <td># SQL analytics course with DuckDB - strings t...</td>\n",
       "      <td>[-0.021185221, -0.0025146175, 0.01119325, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_title  \\\n",
       "0                                    fastapi crud app   \n",
       "1   sql analytics course with duckdb - set theory ...   \n",
       "2                                 python fundamentals   \n",
       "3                                 packaging in python   \n",
       "4                               pydantic fundamentals   \n",
       "5   sql analytics course with duckdb - joins with ...   \n",
       "6   pydantic with gemini to structure output in a ...   \n",
       "7                                  pydanticai chatbot   \n",
       "8   sql analytics course with duckdb - joins concepts   \n",
       "9                                docker setup windows   \n",
       "10  sql analytics course with duckdb - sakila bi d...   \n",
       "11    sql analytics course with duckdb - setup duckdb   \n",
       "12  sql analytics course with duckdb - strings con...   \n",
       "13                         logistic regression theory   \n",
       "14                                  api trafiklab (1)   \n",
       "15  sql analytics course with duckdb - pandas and ...   \n",
       "16  sql analytics course with duckdb - views tutorial   \n",
       "17                                  pandas_read_excel   \n",
       "18                                       python intro   \n",
       "19                                pytest unit testing   \n",
       "20  sql analytics course with duckdb - sakila bi d...   \n",
       "21                     data platform course structure   \n",
       "22        course structure for azure two weeks course   \n",
       "23   modern data stack - dockerize your data pipeline   \n",
       "24       xgboost hands on tutorial for classification   \n",
       "25                                  data storytelling   \n",
       "26                                      api trafiklab   \n",
       "27  sql analytics course with duckdb - course stru...   \n",
       "28  fastapi and scikit-learn api connect to stream...   \n",
       "29  sql analytics course with duckdb - crud operat...   \n",
       "30  sql analytics course with duckdb - pandas and ...   \n",
       "31  pydanticai fundamentals - outputting structure...   \n",
       "32       chat with your excel data - xlwings lite (1)   \n",
       "33                             dbt modeling snowflake   \n",
       "34  serving pydanticai gemini model with fastapi t...   \n",
       "35                                      postgres sink   \n",
       "36                                       python_oop_1   \n",
       "37           chat with your excel data - xlwings lite   \n",
       "38     an introduction to the vector database lancedb   \n",
       "39  serving pydanticai gemini model with fastapi t...   \n",
       "40                                    terraform setup   \n",
       "41           sql analytics with duckdb - introduction   \n",
       "42  sql analytics course with duckdb - set theory ...   \n",
       "43                  data processing course  structure   \n",
       "44  modern data stack - deploy dockerized dashboar...   \n",
       "45  sql analytics course with duckdb - subquery tu...   \n",
       "46     logistic regression hands on with scikit learn   \n",
       "47  sql analytics course with duckdb - dlt to load...   \n",
       "48  modern data stack - using dlt to extract and l...   \n",
       "49                            hands on regularization   \n",
       "50              azure static web app deploy react app   \n",
       "51                                 how does llm work_   \n",
       "52  sql analytics course with duckdb - strings tut...   \n",
       "\n",
       "                                              content  \\\n",
       "0   # Fastapi CRUD app\\n\\n**Kokchun Giang:** [00:0...   \n",
       "1   # SQL analytics course with DuckDB - set theor...   \n",
       "2   # Python fundamentals\\n\\n[00:00:00] Hello and ...   \n",
       "3   # Packaging in python\\n\\n[00:00:00] Hello and ...   \n",
       "4   # Pydantic fundamentals\\n\\n[00:00:00] Hello an...   \n",
       "5   # SQL analytics course with DuckDB - joins wit...   \n",
       "6   # Pydantic with gemini to structure output in ...   \n",
       "7   # pydanticAI chatbot\\n\\n[00:00:00] Hello and w...   \n",
       "8   # SQL analytics course with DuckDB - joins con...   \n",
       "9   # docker setup windows\\n\\n[00:00:00] Hello and...   \n",
       "10  # SQL analytics course with DuckDB - Sakila BI...   \n",
       "11  # SQL analytics course with DuckDB - setup duc...   \n",
       "12  # SQL analytics course with DuckDB - strings c...   \n",
       "13  # Logistic regression theory\\n\\n[00:00:00] Hel...   \n",
       "14  # API trafiklab\\n\\n[00:00:00] Hello and welcom...   \n",
       "15  # SQL analytics course with DuckDB - pandas an...   \n",
       "16  # SQL analytics course with DuckDB - views tut...   \n",
       "17  # pandas\\_read\\_excel\\n\\n[00:00:00] Hello and ...   \n",
       "18  # python intro\\n\\n[00:00:00] Hello and welcome...   \n",
       "19  # pytest unit testing\\n\\n[00:00:00] Hello and ...   \n",
       "20  # SQL analytics course with DuckDB - Sakila BI...   \n",
       "21  # Data platform course structure\\n\\n[00:00:00]...   \n",
       "22  # Course structure for Azure two weeks course\\...   \n",
       "23  # Modern data stack - dockerize your data pipe...   \n",
       "24  # XGBoost hands on tutorial for classification...   \n",
       "25  # data storytelling\\n\\n[00:00:00] Hello and we...   \n",
       "26  # API trafiklab\\n\\n[00:00:00] Hello and welcom...   \n",
       "27  # SQL analytics course with DuckDB - course st...   \n",
       "28  # FastAPI and scikit-learn API connect to stre...   \n",
       "29  # SQL analytics course with DuckDB - CRUD oper...   \n",
       "30  # SQL analytics course with DuckDB - pandas an...   \n",
       "31  # PydanticAI fundamentals - outputting structu...   \n",
       "32  # Chat with your excel data - xlwings lite\\n\\n...   \n",
       "33  # dbt modeling snowflake\\n\\n**Kokchun Giang:**...   \n",
       "34  # Serving PydanticAI Gemini Model with FastAPI...   \n",
       "35  # postgres sink\\n\\n[00:00:00] Hello and welcom...   \n",
       "36  # Python\\_oop\\_1\\n\\n**kokchun:** [00:00:00] He...   \n",
       "37  # Chat with your excel data - xlwings lite\\n\\n...   \n",
       "38  # An introduction to the vector database Lance...   \n",
       "39  # Serving PydanticAI Gemini Model with FastAPI...   \n",
       "40  # Terraform setup\\n\\n[00:00:00] Hello and welc...   \n",
       "41  # SQL analytics with DuckDB - introduction\\n\\n...   \n",
       "42  # SQL analytics course with DuckDB - set theor...   \n",
       "43  # data processing course structure\\n\\n**Kokchu...   \n",
       "44  # Modern data stack - deploy dockerized dashbo...   \n",
       "45  # SQL analytics course with DuckDB - subquery ...   \n",
       "46  # Logistic regression hands on with scikit lea...   \n",
       "47  # SQL analytics course with DuckDB - dlt to lo...   \n",
       "48  # Modern data stack - using dlt to extract and...   \n",
       "49  # Hands on regularization\\n\\n[00:00:00] Hello ...   \n",
       "50  # Azure static web app deploy react app\\n\\n[00...   \n",
       "51  # How does LLM work?\\n\\n[00:00:00] Hello and w...   \n",
       "52  # SQL analytics course with DuckDB - strings t...   \n",
       "\n",
       "                                            embedding  \n",
       "0   [-0.012556567, -0.01593715, 0.022671303, -0.07...  \n",
       "1   [-0.027204884, -0.023552606, 0.019531347, -0.0...  \n",
       "2   [-0.011976539, 0.0025688808, 0.015519834, -0.0...  \n",
       "3   [-0.0150246415, 0.0042777252, 0.038052212, -0....  \n",
       "4   [-0.017777685, -0.01204192, 0.02006065, -0.059...  \n",
       "5   [-0.022949534, -0.022863599, 0.013395227, -0.0...  \n",
       "6   [-0.01512796, -0.003907627, 0.015176425, -0.07...  \n",
       "7   [-0.012143856, -0.001205422, 0.013563879, -0.0...  \n",
       "8   [-0.027722066, -0.016966412, 0.012043696, -0.0...  \n",
       "9   [-0.0034482135, 0.010729573, 0.016939094, -0.0...  \n",
       "10  [0.0055291746, -0.004573792, 0.019058505, -0.0...  \n",
       "11  [-0.008761382, -0.0034869136, 0.01760935, -0.0...  \n",
       "12  [-0.02525857, -0.011639317, 0.004399808, -0.07...  \n",
       "13  [-0.008146983, -0.00016716555, 0.011266027, -0...  \n",
       "14  [-0.006812348, -0.0044766036, 0.014319714, -0....  \n",
       "15  [-0.009578243, -0.011996515, 0.012965736, -0.0...  \n",
       "16  [-0.027096331, -0.0073001576, 0.0048975083, -0...  \n",
       "17  [-0.0038475876, 0.0054737586, 0.024301918, -0....  \n",
       "18  [-0.0110368235, -0.0020397531, 0.012955041, -0...  \n",
       "19  [-0.014343338, 0.0007622975, 0.030120881, -0.0...  \n",
       "20  [0.0055291746, -0.004573792, 0.019058505, -0.0...  \n",
       "21  [0.010435624, -0.0016020014, 0.011163727, -0.0...  \n",
       "22  [0.0015313567, 0.012794174, 0.016358217, -0.07...  \n",
       "23  [0.0027987233, 0.014228618, 0.027441906, -0.09...  \n",
       "24  [-0.004887973, 0.008207077, 0.013008361, -0.09...  \n",
       "25  [-0.011316549, 0.016874935, 0.012392512, -0.08...  \n",
       "26  [-0.006812348, -0.0044766036, 0.014319714, -0....  \n",
       "27  [0.0023245383, -0.0029253534, 0.0036489798, -0...  \n",
       "28  [-0.017560659, 0.009985835, 0.01749353, -0.085...  \n",
       "29  [-0.018357037, -0.0009783215, 0.025988214, -0....  \n",
       "30  [-0.009578243, -0.011996515, 0.012965736, -0.0...  \n",
       "31  [-0.033087507, 0.0036856267, 0.02514256, -0.07...  \n",
       "32  [-0.0071265996, 0.020476552, 0.017235534, -0.0...  \n",
       "33  [-0.010636117, 0.01212832, 0.029030465, -0.086...  \n",
       "34  [-0.018388461, -0.006911759, 0.015190295, -0.0...  \n",
       "35  [0.007228276, 0.021961471, 0.031665433, -0.088...  \n",
       "36  [-0.008043373, -0.011165032, 0.031197485, -0.0...  \n",
       "37  [-0.0071265996, 0.020476552, 0.017235534, -0.0...  \n",
       "38  [-0.038686633, 0.0036908067, 0.02178414, -0.07...  \n",
       "39  [-0.018388461, -0.006911759, 0.015190295, -0.0...  \n",
       "40  [-0.004057311, 0.01868715, 0.01563467, -0.0808...  \n",
       "41  [-0.028552804, -0.0118231885, 0.00639494, -0.0...  \n",
       "42  [-0.027204884, -0.023552606, 0.019531347, -0.0...  \n",
       "43  [0.0032914313, 0.01059015, 0.009196502, -0.064...  \n",
       "44  [-0.0075815883, 0.02116889, 0.027429571, -0.10...  \n",
       "45  [-0.012517108, -0.014230472, -0.0019505646, -0...  \n",
       "46  [-0.0066825696, 0.002244388, 0.011106265, -0.0...  \n",
       "47  [-0.0066205882, -0.0075128363, 0.019882608, -0...  \n",
       "48  [-0.00826649, 0.019363934, 0.016988434, -0.080...  \n",
       "49  [-0.009709013, 6.357031e-05, 0.023912106, -0.0...  \n",
       "50  [-0.012295628, 0.015832268, 0.025340993, -0.09...  \n",
       "51  [-0.027908303, 0.017137818, 0.023856219, -0.06...  \n",
       "52  [-0.021185221, -0.0025146175, 0.01119325, -0.0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"transcriptions\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63be76da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "      <th>_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how does llm work_</td>\n",
       "      <td># How does LLM work?\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.027908303, 0.017137818, 0.023856219, -0.06...</td>\n",
       "      <td>0.549269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pydanticai fundamentals - outputting structure...</td>\n",
       "      <td># PydanticAI fundamentals - outputting structu...</td>\n",
       "      <td>[-0.033087507, 0.0036856267, 0.02514256, -0.07...</td>\n",
       "      <td>0.690942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pydanticai chatbot</td>\n",
       "      <td># pydanticAI chatbot\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.012143856, -0.001205422, 0.013563879, -0.0...</td>\n",
       "      <td>0.692687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0                                 how does llm work_   \n",
       "1  pydanticai fundamentals - outputting structure...   \n",
       "2                                 pydanticai chatbot   \n",
       "\n",
       "                                             content  \\\n",
       "0  # How does LLM work?\\n\\n[00:00:00] Hello and w...   \n",
       "1  # PydanticAI fundamentals - outputting structu...   \n",
       "2  # pydanticAI chatbot\\n\\n[00:00:00] Hello and w...   \n",
       "\n",
       "                                           embedding  _distance  \n",
       "0  [-0.027908303, 0.017137818, 0.023856219, -0.06...   0.549269  \n",
       "1  [-0.033087507, 0.0036856267, 0.02514256, -0.07...   0.690942  \n",
       "2  [-0.012143856, -0.001205422, 0.013563879, -0.0...   0.692687  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Learn about LLM\"\n",
    "search_result = vector_db[\"transcriptions\"].search(query).limit(3).to_pandas()\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427ecdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# How does LLM work?\\n\\n[00:00:00] Hello and welcome to this video where we'll go into large language models and how do they work. We'll go into the theory and intuition about how models like chat, g, pt, Gemini, Claude, et cetera, how they work, the fundamental theory about it. ~~We will go into to get. ~~To give you intuition on how does it generate language, how can it understand language, et cetera.\\n\\nThis will be really interesting and hopefully that you'll get some intuition. You don't need to understand exactly the details, how it works. However, some basic intuition is good that you, you know that it's not magic, it's just mathematics and statistics. However, we won't go into much math and statistics\\n\\nwe will keep it simple and that you can understand it. Yes. Let's move on to the slides.\\n\\n**Kokchun Giang-1:** how does she GPT and other language models work? An introduction to large language models? How can we start with [00:01:00] text? How can we represent text for a computer? A naive approach. Start to understand that the computer, it can only understand numbers, it can only understand zero and ones. We need to somehow represent.\\n\\nThe text into numbers that the computer can understand it. This is our starting point. Computer doesn't understand text. It understands numbers, which can be represented in binary as ones and zeros. We need to represent text with numbers. This is our starting point. Suppose our vocabulary has these words.\\n\\nWe have, Hey, ~~Canon ~~fis do, sorry, this is in Swedish, but these are just a few words in Swedish vocabulary. Then one hot encoded vector would look like this. Have a vector. This means that this represent hay. If this is our vocabulary, and then we have zero on all the other ones. This is just the [00:02:00] word, Hey, ~~hay ~~can be represented with this.\\n\\nIn Swedish there's 126,000 words. Then we'll have a very huge vector if we will represent it as in this way. This is a term based approach to represent the words. Then we'll have a one, somewhere to represent one word, and then zero everywhere else. Is this a good approach?\\n\\nThat is the question. Get the very, we'll, get the very sparse vector. That means that we have a lot of zeros and just one, one, but we also want to have some semantic meaning between the words. What does the semantic meaning mean? For example, we have the word hello and the word bi. They should be somewhat close to each other as both are about greeting.\\n\\nHow are you, maybe it should be close as well. While a rabbit and a dog should be somewhat close to each other as both [00:03:00] our pets, a rabbit and a hare should be closer than a rabbit and a dog because they may look. More similar, they have more features that are similar and fish and goldfish should be very similar.\\n\\nThis is what we mean with semantic meaning. This is not captured by using this naive approach as we will only have a one somewhere since we have a rabbit and a dog. They are quite. Far away from each other in the alphabet. Then it means that~~ they, ~~they're not related at all~~ in ~~in the naive approach, but we want to have them more relatable that we can start to get the grasp of the language to start model and understand the language.\\n\\nThen we'll go further into embeddings. Then 2013 there was ~~a, ~~an article called Word to ve, how to Represent Words with Vector Embeddings that captures semantic meaning. This is a kind of the start [00:04:00] here. Here I have ~~a. ~~A Cartesian coordinate system where we have a size as a feature and we have loves hay as another feature.\\n\\nThis is just 2D but we could expand this to many dimension, but it's impossible to show it. We have here a tiger that ~~is, ~~has a large size, but it doesn't love hay. While we have a rabbit here that is smaller size but loves hay. Where does cow and calf go? That is the question. You can, for example, say that cow, it's quite large in size.\\n\\nYou can say two here. And then it also loves hay. It eats hay. While the calf, it's somewhat smaller in size, but also loves hay. You can see that, and they are closer. They are ~~close to they are in ~~in this place here. This is how you can represent them. These are vectors.\\n\\nYou can draw lines or you can draw arrow to these, and you get vectors. [00:05:00] However, we have larger dimensional embedding vectors of course, because these embedding vectors, they become larger and larger when you have more and more features. ~~If we could have more. Size and love, say we could have domesticated as one feature.~~\\n\\nWe could have for example, mammal as a feature we could have living in water as a feature, et cetera. We could have a lot of features. ~~We can have yeah, there's a lot. ~~To find similar words, we use a dot product, which gives co-sign similarity between the vectors. ~~We take we find the co-sign similarity.~~\\n\\nThe cosign similarity is basically just an angle. We ~~took, ~~take a look at the angles between two vectors and see which one is closest. That is the idea to ~~get then we ~~get the semantic meaning. We can also do an interesting thing is that we can do some calculations with the vectors.\\n\\nWe could add vectors and subtract vectors to get different types of results. That is quite cool. Moving on. In 2017, we have this paper. Attention is all you need. This is the [00:06:00] Transformers architecture, and this is what all language models are based on as of today. We start here, the goal is to predict the next word based on the previous sequence.\\n\\nYou have a sequence of words and you want to predict the next word that is the goal. For example, here in Swedish, there's who I lag it is how are you and who more do if there's two are, then there's lag. Who more than is do it is like, how are, how is. How are you? For this we need to understand the context through the attention.\\n\\nFor example, I am cool. The cool it refers to I, then it means that it's the coolness, I am cool. However, the ice cream is cool. It's referring to the ice cream. If it's referring to the ice cream, then this cool, it means something else. ~~Then this cool up here. ~~This cool [00:07:00] is referring to the person I, and this cool is referring to the ice cream where it's.\\n\\nCool as in low temperature. Depending on the context the same word could have different meanings. This is where the attention is. ~~All you need is very very neat. ~~Also here it goes away from previously it was used in RNNA lot recurrent neural network, but here it's go, it's removing that all along.\\n\\nHere we compute similarities to see which words that determine the context for cool as it has different meanings in different contexts. ~~Computing similarities to see. The words. Okay. Similarities. ~~With the transformer, we can generate text word by word using previous words as contexts. It's jumping around and see the different similarities, scores and see that.\\n\\nOkay. Is this one that will determine. Cool the most while death and is not determining cool as much. How does it know what is determining it? It depends [00:08:00] on all the training data that this has gone. You have sent in a lot of training data and it has learned this patents. Now we can generate text word by word using previous words as context.\\n\\nFor example, like this, I am cool. I am, and then it looks into its training data and sees like cool, appears a lot of time in the training data, and then it'll predict cool with the highest probability. Then afterwards it predicts the word yo because that is also very common in this training data set that I have thought about in my mind.\\n\\nThen I am cool. Yo, it comes out. I'm cool. Yo Zap then it's yeah, depending on this training data set. As you can see this what it does is that it. Takes the sequence, send it in, and it'll predict. Cool. Then it'll send in this next sequence with I am Cool. It'll predict yo and we send in all this and then it'll [00:09:00] predict sap.\\n\\nThis is a little bit of simplification. It's not the really words that it predicts. It predicts something called tokens, but it's more more division of the word in different ways. But we don't need to go into that for simplicity. We'll think about it as words. Okay. Then we'll get a little bit further in time.\\n\\nWe get into GPT general pre-trained Transformers, and then this is trained with unsupervised learning on large corpus of text ~~is pre-training, it's trained on. The data ~~from the internet, basically. Using Internet's text, it can predict most probable next word to generate. Then when you train on much data, it become, it becomes something called emergent capability.\\n\\nIt's start to be able to find patterns ~~that you are not ~~that people are not sure about before. Then you can also add. Something called temperature, and we'll get variations and [00:10:00] creativity. What this means is that maybe ~~we don't need to pre, ~~we don't want to predict I am cool instead maybe it predicts I am super because super maybe was the third most probable word.\\n\\nBut since we have temperature, it becomes like, I don't need to pick the most probable word. I can pick some other word with some probability. The higher temperature it gets, the more ~~it. ~~It becomes more wild. That is to say it can pick other words ~~in its core, ~~in its training. Then ~~when ~~we can fine tune it to specific task using supervised learning that you can, for example, chat in a certain format.\\n\\nYou need to train it with a certain format that it should answer in this type of format. Here we have a supervised learning. Then in order to get it to become good to answer in a way that we want it answer, we have something called reinforcement learning with human feedback, [00:11:00] RLHF.\\n\\nWe let the model answer a set of question several times, you can generate a lot of different answers. We have different temperatures, we get different types of answers, and then we let. A lot of humans very lowly paid humans, unfortunately that will score these answers and feed back to the system how good these answers were.\\n\\nThe systems will try to maximize its scores, it gets scores, for example, it, you, it says one a type of answer and you give it back a score of four. But in a scale of 10, and then another answer, it gives seven another answer, five, et cetera, and it'll try to get the highest score all the time to satisfy the humans.\\n\\nThe system will try to maximize its scores. Example, an answer of how to make a bomb will get low scores, for example, and model will try predict which type of answers that humans [00:12:00] like. Then, after RLHF, ~~we have ~~we have come to the part of we have a chat, ~~GPT three point ~~GPT, 3.5. It's like the first chat, GPT to make it very simple.\\n\\nI've skipped a lot, I've skipped some of the details. But this I think it's the intuition that you need to see that. Yes, now we have a model that we can chat with and ~~then ~~then there's a lot of different improvements in a lot of different models and and then a lot of variations as well that I haven't covered.\\n\\nBut I want to cover one more thing is that we want to make the model become more. Capable and understand more different things and ~~do ~~be able to perform different things. For example, we can send in a video and it'll understand it. ~~We, ~~it can produce a video, it can produce an image, we can send in an image and it'll understand that, et cetera.\\n\\nHow to do that is that we've gone into large multimodal models. LMM. Here LLMs [00:13:00] are language models and not good at, for example, math. It cannot do computation very well because ~~it's, ~~that's not its main purpose. Also it can't answer questions on things that happens after its training cutoff.\\n\\nThen what you can do is that it can however, use tools to handle this. For example, we can use tools such as a calculator to do computations because that will be much better than using. The raw training data that we have, and we can use Google search to enhance our context.\\n\\nWe can get more context of the things that ~~is ~~outside of our cutoff or more specific things that it becomes visible for the model to use to generate an answer. We can have an image generator to generate image ~~when ~~when someone asks for an image. We can also send, when someone sends an image, we can use~~ i ~~other AI tools such as image recognitions to try to understand the image ~~to ~~to generate text based on it and et [00:14:00] cetera with video ~~et.~~\\n\\nLMS can also process and understand other inputs such as video, image, audio, et cetera. You can talk to it, it can generate back speech with text to speech. And using the newer it becomes more and more natural as well ~~in, in the text. ~~In the speech. ~~Yes, I.~~\\n\\nin this video we have gone through LLMs and how do they work and to get some intuition about how it works. We have gone into ~~some ~~some very brief concepts of different parts for an LLM to work and but not gone into all the details. Of course, there's too much and we have tried to skip.\\n\\nSome of the math that it becomes quite simple to understand. Starting with like how to represent text and then going more towards training on how the attention works. Then going into training on the internet data to get pre-trained, GPT and then going further into supervised learning and reinforcement learning with human feedback that we [00:15:00] can get the chat.\\n\\nChatting format that we are used to and then moving further into ~~large ~~large multimodal lang models where it can be more capable to use different types of tools and understand different type of inputs and getting different types of outputs. I hope that you have learned a lot in this video ~~and ~~and see you in the next one.\\n\\nBye.\\n\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLM prompt\n",
    "response = vector_db[\"transcriptions\"].search(\"LLM\").to_list()[0][\"content\"]\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-lab-de24-john-sandsjo (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
