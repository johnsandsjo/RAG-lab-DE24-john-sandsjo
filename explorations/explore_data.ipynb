{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572afbab",
   "metadata": {},
   "source": [
    "# Explore the data in vectore db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d77cba88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceDBConnection(uri='/Users/john.sandsjo/Documents/github/RAG-lab-DE24-john-sandsjo/transcript_repo')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "from pathlib import Path\n",
    "\n",
    "VECTOR_DB_PATH = Path.cwd().parent / \"transcript_repo\"\n",
    "vector_db = lancedb.connect(uri= VECTOR_DB_PATH)\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d4c3ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transcriptions']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a16f3df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceTable(name='transcriptions', version=83, _conn=LanceDBConnection(uri='/Users/john.sandsjo/Documents/github/RAG-lab-DE24-john-sandsjo/transcript_repo'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"transcriptions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff325473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_link</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fastapi crud app</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Fastapi CRUD app\\n\\n**Kokchun Giang:** [00:0...</td>\n",
       "      <td>[-0.012556567, -0.01593715, 0.022671303, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sql analytics course with duckdb - set theory ...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - set theor...</td>\n",
       "      <td>[-0.027204884, -0.023552606, 0.019531347, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python fundamentals</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Python fundamentals\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.011976539, 0.0025688808, 0.015519834, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>packaging in python</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Packaging in python\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.0150246415, 0.0042777252, 0.038052212, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pydantic fundamentals</td>\n",
       "      <td>https://www.youtube.com/watch?v=hHCMUc3gv40</td>\n",
       "      <td># Pydantic fundamentals\\n\\n[00:00:00] Hello an...</td>\n",
       "      <td>[-0.017777685, -0.01204192, 0.02006065, -0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sql analytics course with duckdb - joins with ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=RjPen0FavwU</td>\n",
       "      <td># SQL analytics course with DuckDB - joins wit...</td>\n",
       "      <td>[-0.022949534, -0.022863599, 0.013395227, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pydantic with gemini to structure output in a ...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Pydantic with gemini to structure output in ...</td>\n",
       "      <td>[-0.01512796, -0.003907627, 0.015176425, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pydanticai chatbot</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># pydanticAI chatbot\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.012143856, -0.001205422, 0.013563879, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sql analytics course with duckdb - joins concepts</td>\n",
       "      <td>https://www.youtube.com/watch?v=LcI6GvmpuYA</td>\n",
       "      <td># SQL analytics course with DuckDB - joins con...</td>\n",
       "      <td>[-0.027722066, -0.016966412, 0.012043696, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>docker setup windows</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># docker setup windows\\n\\n[00:00:00] Hello and...</td>\n",
       "      <td>[-0.0034482135, 0.010729573, 0.016939094, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sql analytics course with duckdb - sakila bi d...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - Sakila BI...</td>\n",
       "      <td>[0.0055291746, -0.004573792, 0.019058505, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sql analytics course with duckdb - setup duckdb</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - setup duc...</td>\n",
       "      <td>[-0.008761382, -0.0034869136, 0.01760935, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sql analytics course with duckdb - strings con...</td>\n",
       "      <td>https://www.youtube.com/watch?v=F9I7aM4k9ug</td>\n",
       "      <td># SQL analytics course with DuckDB - strings c...</td>\n",
       "      <td>[-0.02525857, -0.011639317, 0.004399808, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logistic regression theory</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Logistic regression theory\\n\\n[00:00:00] Hel...</td>\n",
       "      <td>[-0.008146983, -0.00016716555, 0.011266027, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>api trafiklab (1)</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># API trafiklab\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[-0.006812348, -0.0044766036, 0.014319714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sql analytics course with duckdb - pandas and ...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - pandas an...</td>\n",
       "      <td>[-0.009578243, -0.011996515, 0.012965736, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sql analytics course with duckdb - views tutorial</td>\n",
       "      <td>https://www.youtube.com/watch?v=v2k6ZZz23oc</td>\n",
       "      <td># SQL analytics course with DuckDB - views tut...</td>\n",
       "      <td>[-0.027096331, -0.0073001576, 0.0048975083, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pandas_read_excel</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># pandas\\_read\\_excel\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.0038475876, 0.0054737586, 0.024301918, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>python intro</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># python intro\\n\\n[00:00:00] Hello and welcome...</td>\n",
       "      <td>[-0.0110368235, -0.0020397531, 0.012955041, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pytest unit testing</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># pytest unit testing\\n\\n[00:00:00] Hello and ...</td>\n",
       "      <td>[-0.014343338, 0.0007622975, 0.030120881, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sql analytics course with duckdb - sakila bi d...</td>\n",
       "      <td>https://www.youtube.com/watch?v=8jMIRtGwReY</td>\n",
       "      <td># SQL analytics course with DuckDB - Sakila BI...</td>\n",
       "      <td>[0.0055291746, -0.004573792, 0.019058505, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data platform course structure</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Data platform course structure\\n\\n[00:00:00]...</td>\n",
       "      <td>[0.010435624, -0.0016020014, 0.011163727, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>course structure for azure two weeks course</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Course structure for Azure two weeks course\\...</td>\n",
       "      <td>[0.0015313567, 0.012794174, 0.016358217, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>modern data stack - dockerize your data pipeline</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Modern data stack - dockerize your data pipe...</td>\n",
       "      <td>[0.0027987233, 0.014228618, 0.027441906, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>xgboost hands on tutorial for classification</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># XGBoost hands on tutorial for classification...</td>\n",
       "      <td>[-0.004887973, 0.008207077, 0.013008361, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>data storytelling</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># data storytelling\\n\\n[00:00:00] Hello and we...</td>\n",
       "      <td>[-0.011316549, 0.016874935, 0.012392512, -0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>api trafiklab</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># API trafiklab\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[-0.006812348, -0.0044766036, 0.014319714, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sql analytics course with duckdb - course stru...</td>\n",
       "      <td>https://www.youtube.com/watch?v=weC950i58Gs</td>\n",
       "      <td># SQL analytics course with DuckDB - course st...</td>\n",
       "      <td>[0.0023245383, -0.0029253534, 0.0036489798, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fastapi and scikit-learn api connect to stream...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># FastAPI and scikit-learn API connect to stre...</td>\n",
       "      <td>[-0.017560659, 0.009985835, 0.01749353, -0.085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sql analytics course with duckdb - crud operat...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - CRUD oper...</td>\n",
       "      <td>[-0.018357037, -0.0009783215, 0.025988214, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sql analytics course with duckdb - pandas and ...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - pandas an...</td>\n",
       "      <td>[-0.009578243, -0.011996515, 0.012965736, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pydanticai fundamentals - outputting structure...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># PydanticAI fundamentals - outputting structu...</td>\n",
       "      <td>[-0.033087507, 0.0036856267, 0.02514256, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>chat with your excel data - xlwings lite (1)</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Chat with your excel data - xlwings lite\\n\\n...</td>\n",
       "      <td>[-0.0071265996, 0.020476552, 0.017235534, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>dbt modeling snowflake</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># dbt modeling snowflake\\n\\n**Kokchun Giang:**...</td>\n",
       "      <td>[-0.010636117, 0.01212832, 0.029030465, -0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>serving pydanticai gemini model with fastapi t...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Serving PydanticAI Gemini Model with FastAPI...</td>\n",
       "      <td>[-0.018388461, -0.006911759, 0.015190295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>postgres sink</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># postgres sink\\n\\n[00:00:00] Hello and welcom...</td>\n",
       "      <td>[0.007228276, 0.021961471, 0.031665433, -0.088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>python_oop_1</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Python\\_oop\\_1\\n\\n**kokchun:** [00:00:00] He...</td>\n",
       "      <td>[-0.008043373, -0.011165032, 0.031197485, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>chat with your excel data - xlwings lite</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Chat with your excel data - xlwings lite\\n\\n...</td>\n",
       "      <td>[-0.0071265996, 0.020476552, 0.017235534, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>an introduction to the vector database lancedb</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># An introduction to the vector database Lance...</td>\n",
       "      <td>[-0.038686633, 0.0036908067, 0.02178414, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>serving pydanticai gemini model with fastapi t...</td>\n",
       "      <td>https://www.youtube.com/watch?v=mp650k5T2T8</td>\n",
       "      <td># Serving PydanticAI Gemini Model with FastAPI...</td>\n",
       "      <td>[-0.018388461, -0.006911759, 0.015190295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>terraform setup</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Terraform setup\\n\\n[00:00:00] Hello and welc...</td>\n",
       "      <td>[-0.004057311, 0.01868715, 0.01563467, -0.0808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sql analytics with duckdb - introduction</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics with DuckDB - introduction\\n\\n...</td>\n",
       "      <td>[-0.028552804, -0.0118231885, 0.00639494, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sql analytics course with duckdb - set theory ...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - set theor...</td>\n",
       "      <td>[-0.027204884, -0.023552606, 0.019531347, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>data processing course  structure</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># data processing course structure\\n\\n**Kokchu...</td>\n",
       "      <td>[0.0032914313, 0.01059015, 0.009196502, -0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>modern data stack - deploy dockerized dashboar...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Modern data stack - deploy dockerized dashbo...</td>\n",
       "      <td>[-0.0075815883, 0.02116889, 0.027429571, -0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>sql analytics course with duckdb - subquery tu...</td>\n",
       "      <td>https://www.youtube.com/watch?v=aZQtKjWfaDw</td>\n",
       "      <td># SQL analytics course with DuckDB - subquery ...</td>\n",
       "      <td>[-0.012517108, -0.014230472, -0.0019505646, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>logistic regression hands on with scikit learn</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Logistic regression hands on with scikit lea...</td>\n",
       "      <td>[-0.0066825696, 0.002244388, 0.011106265, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>sql analytics course with duckdb - dlt to load...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - dlt to lo...</td>\n",
       "      <td>[-0.0066205882, -0.0075128363, 0.019882608, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>modern data stack - using dlt to extract and l...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Modern data stack - using dlt to extract and...</td>\n",
       "      <td>[-0.00826649, 0.019363934, 0.016988434, -0.080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>hands on regularization</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Hands on regularization\\n\\n[00:00:00] Hello ...</td>\n",
       "      <td>[-0.009709013, 6.357031e-05, 0.023912106, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>azure static web app deploy react app</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># Azure static web app deploy react app\\n\\n[00...</td>\n",
       "      <td>[-0.012295628, 0.015832268, 0.025340993, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>how does llm work_</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># How does LLM work?\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.027908303, 0.017137818, 0.023856219, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>sql analytics course with duckdb - strings tut...</td>\n",
       "      <td>https://www.youtube.com/@AIgineer</td>\n",
       "      <td># SQL analytics course with DuckDB - strings t...</td>\n",
       "      <td>[-0.021185221, -0.0025146175, 0.01119325, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_title  \\\n",
       "0                                    fastapi crud app   \n",
       "1   sql analytics course with duckdb - set theory ...   \n",
       "2                                 python fundamentals   \n",
       "3                                 packaging in python   \n",
       "4                               pydantic fundamentals   \n",
       "5   sql analytics course with duckdb - joins with ...   \n",
       "6   pydantic with gemini to structure output in a ...   \n",
       "7                                  pydanticai chatbot   \n",
       "8   sql analytics course with duckdb - joins concepts   \n",
       "9                                docker setup windows   \n",
       "10  sql analytics course with duckdb - sakila bi d...   \n",
       "11    sql analytics course with duckdb - setup duckdb   \n",
       "12  sql analytics course with duckdb - strings con...   \n",
       "13                         logistic regression theory   \n",
       "14                                  api trafiklab (1)   \n",
       "15  sql analytics course with duckdb - pandas and ...   \n",
       "16  sql analytics course with duckdb - views tutorial   \n",
       "17                                  pandas_read_excel   \n",
       "18                                       python intro   \n",
       "19                                pytest unit testing   \n",
       "20  sql analytics course with duckdb - sakila bi d...   \n",
       "21                     data platform course structure   \n",
       "22        course structure for azure two weeks course   \n",
       "23   modern data stack - dockerize your data pipeline   \n",
       "24       xgboost hands on tutorial for classification   \n",
       "25                                  data storytelling   \n",
       "26                                      api trafiklab   \n",
       "27  sql analytics course with duckdb - course stru...   \n",
       "28  fastapi and scikit-learn api connect to stream...   \n",
       "29  sql analytics course with duckdb - crud operat...   \n",
       "30  sql analytics course with duckdb - pandas and ...   \n",
       "31  pydanticai fundamentals - outputting structure...   \n",
       "32       chat with your excel data - xlwings lite (1)   \n",
       "33                             dbt modeling snowflake   \n",
       "34  serving pydanticai gemini model with fastapi t...   \n",
       "35                                      postgres sink   \n",
       "36                                       python_oop_1   \n",
       "37           chat with your excel data - xlwings lite   \n",
       "38     an introduction to the vector database lancedb   \n",
       "39  serving pydanticai gemini model with fastapi t...   \n",
       "40                                    terraform setup   \n",
       "41           sql analytics with duckdb - introduction   \n",
       "42  sql analytics course with duckdb - set theory ...   \n",
       "43                  data processing course  structure   \n",
       "44  modern data stack - deploy dockerized dashboar...   \n",
       "45  sql analytics course with duckdb - subquery tu...   \n",
       "46     logistic regression hands on with scikit learn   \n",
       "47  sql analytics course with duckdb - dlt to load...   \n",
       "48  modern data stack - using dlt to extract and l...   \n",
       "49                            hands on regularization   \n",
       "50              azure static web app deploy react app   \n",
       "51                                 how does llm work_   \n",
       "52  sql analytics course with duckdb - strings tut...   \n",
       "\n",
       "                                     video_link  \\\n",
       "0             https://www.youtube.com/@AIgineer   \n",
       "1             https://www.youtube.com/@AIgineer   \n",
       "2             https://www.youtube.com/@AIgineer   \n",
       "3             https://www.youtube.com/@AIgineer   \n",
       "4   https://www.youtube.com/watch?v=hHCMUc3gv40   \n",
       "5   https://www.youtube.com/watch?v=RjPen0FavwU   \n",
       "6             https://www.youtube.com/@AIgineer   \n",
       "7             https://www.youtube.com/@AIgineer   \n",
       "8   https://www.youtube.com/watch?v=LcI6GvmpuYA   \n",
       "9             https://www.youtube.com/@AIgineer   \n",
       "10            https://www.youtube.com/@AIgineer   \n",
       "11            https://www.youtube.com/@AIgineer   \n",
       "12  https://www.youtube.com/watch?v=F9I7aM4k9ug   \n",
       "13            https://www.youtube.com/@AIgineer   \n",
       "14            https://www.youtube.com/@AIgineer   \n",
       "15            https://www.youtube.com/@AIgineer   \n",
       "16  https://www.youtube.com/watch?v=v2k6ZZz23oc   \n",
       "17            https://www.youtube.com/@AIgineer   \n",
       "18            https://www.youtube.com/@AIgineer   \n",
       "19            https://www.youtube.com/@AIgineer   \n",
       "20  https://www.youtube.com/watch?v=8jMIRtGwReY   \n",
       "21            https://www.youtube.com/@AIgineer   \n",
       "22            https://www.youtube.com/@AIgineer   \n",
       "23            https://www.youtube.com/@AIgineer   \n",
       "24            https://www.youtube.com/@AIgineer   \n",
       "25            https://www.youtube.com/@AIgineer   \n",
       "26            https://www.youtube.com/@AIgineer   \n",
       "27  https://www.youtube.com/watch?v=weC950i58Gs   \n",
       "28            https://www.youtube.com/@AIgineer   \n",
       "29            https://www.youtube.com/@AIgineer   \n",
       "30            https://www.youtube.com/@AIgineer   \n",
       "31            https://www.youtube.com/@AIgineer   \n",
       "32            https://www.youtube.com/@AIgineer   \n",
       "33            https://www.youtube.com/@AIgineer   \n",
       "34            https://www.youtube.com/@AIgineer   \n",
       "35            https://www.youtube.com/@AIgineer   \n",
       "36            https://www.youtube.com/@AIgineer   \n",
       "37            https://www.youtube.com/@AIgineer   \n",
       "38            https://www.youtube.com/@AIgineer   \n",
       "39  https://www.youtube.com/watch?v=mp650k5T2T8   \n",
       "40            https://www.youtube.com/@AIgineer   \n",
       "41            https://www.youtube.com/@AIgineer   \n",
       "42            https://www.youtube.com/@AIgineer   \n",
       "43            https://www.youtube.com/@AIgineer   \n",
       "44            https://www.youtube.com/@AIgineer   \n",
       "45  https://www.youtube.com/watch?v=aZQtKjWfaDw   \n",
       "46            https://www.youtube.com/@AIgineer   \n",
       "47            https://www.youtube.com/@AIgineer   \n",
       "48            https://www.youtube.com/@AIgineer   \n",
       "49            https://www.youtube.com/@AIgineer   \n",
       "50            https://www.youtube.com/@AIgineer   \n",
       "51            https://www.youtube.com/@AIgineer   \n",
       "52            https://www.youtube.com/@AIgineer   \n",
       "\n",
       "                                              content  \\\n",
       "0   # Fastapi CRUD app\\n\\n**Kokchun Giang:** [00:0...   \n",
       "1   # SQL analytics course with DuckDB - set theor...   \n",
       "2   # Python fundamentals\\n\\n[00:00:00] Hello and ...   \n",
       "3   # Packaging in python\\n\\n[00:00:00] Hello and ...   \n",
       "4   # Pydantic fundamentals\\n\\n[00:00:00] Hello an...   \n",
       "5   # SQL analytics course with DuckDB - joins wit...   \n",
       "6   # Pydantic with gemini to structure output in ...   \n",
       "7   # pydanticAI chatbot\\n\\n[00:00:00] Hello and w...   \n",
       "8   # SQL analytics course with DuckDB - joins con...   \n",
       "9   # docker setup windows\\n\\n[00:00:00] Hello and...   \n",
       "10  # SQL analytics course with DuckDB - Sakila BI...   \n",
       "11  # SQL analytics course with DuckDB - setup duc...   \n",
       "12  # SQL analytics course with DuckDB - strings c...   \n",
       "13  # Logistic regression theory\\n\\n[00:00:00] Hel...   \n",
       "14  # API trafiklab\\n\\n[00:00:00] Hello and welcom...   \n",
       "15  # SQL analytics course with DuckDB - pandas an...   \n",
       "16  # SQL analytics course with DuckDB - views tut...   \n",
       "17  # pandas\\_read\\_excel\\n\\n[00:00:00] Hello and ...   \n",
       "18  # python intro\\n\\n[00:00:00] Hello and welcome...   \n",
       "19  # pytest unit testing\\n\\n[00:00:00] Hello and ...   \n",
       "20  # SQL analytics course with DuckDB - Sakila BI...   \n",
       "21  # Data platform course structure\\n\\n[00:00:00]...   \n",
       "22  # Course structure for Azure two weeks course\\...   \n",
       "23  # Modern data stack - dockerize your data pipe...   \n",
       "24  # XGBoost hands on tutorial for classification...   \n",
       "25  # data storytelling\\n\\n[00:00:00] Hello and we...   \n",
       "26  # API trafiklab\\n\\n[00:00:00] Hello and welcom...   \n",
       "27  # SQL analytics course with DuckDB - course st...   \n",
       "28  # FastAPI and scikit-learn API connect to stre...   \n",
       "29  # SQL analytics course with DuckDB - CRUD oper...   \n",
       "30  # SQL analytics course with DuckDB - pandas an...   \n",
       "31  # PydanticAI fundamentals - outputting structu...   \n",
       "32  # Chat with your excel data - xlwings lite\\n\\n...   \n",
       "33  # dbt modeling snowflake\\n\\n**Kokchun Giang:**...   \n",
       "34  # Serving PydanticAI Gemini Model with FastAPI...   \n",
       "35  # postgres sink\\n\\n[00:00:00] Hello and welcom...   \n",
       "36  # Python\\_oop\\_1\\n\\n**kokchun:** [00:00:00] He...   \n",
       "37  # Chat with your excel data - xlwings lite\\n\\n...   \n",
       "38  # An introduction to the vector database Lance...   \n",
       "39  # Serving PydanticAI Gemini Model with FastAPI...   \n",
       "40  # Terraform setup\\n\\n[00:00:00] Hello and welc...   \n",
       "41  # SQL analytics with DuckDB - introduction\\n\\n...   \n",
       "42  # SQL analytics course with DuckDB - set theor...   \n",
       "43  # data processing course structure\\n\\n**Kokchu...   \n",
       "44  # Modern data stack - deploy dockerized dashbo...   \n",
       "45  # SQL analytics course with DuckDB - subquery ...   \n",
       "46  # Logistic regression hands on with scikit lea...   \n",
       "47  # SQL analytics course with DuckDB - dlt to lo...   \n",
       "48  # Modern data stack - using dlt to extract and...   \n",
       "49  # Hands on regularization\\n\\n[00:00:00] Hello ...   \n",
       "50  # Azure static web app deploy react app\\n\\n[00...   \n",
       "51  # How does LLM work?\\n\\n[00:00:00] Hello and w...   \n",
       "52  # SQL analytics course with DuckDB - strings t...   \n",
       "\n",
       "                                            embedding  \n",
       "0   [-0.012556567, -0.01593715, 0.022671303, -0.07...  \n",
       "1   [-0.027204884, -0.023552606, 0.019531347, -0.0...  \n",
       "2   [-0.011976539, 0.0025688808, 0.015519834, -0.0...  \n",
       "3   [-0.0150246415, 0.0042777252, 0.038052212, -0....  \n",
       "4   [-0.017777685, -0.01204192, 0.02006065, -0.059...  \n",
       "5   [-0.022949534, -0.022863599, 0.013395227, -0.0...  \n",
       "6   [-0.01512796, -0.003907627, 0.015176425, -0.07...  \n",
       "7   [-0.012143856, -0.001205422, 0.013563879, -0.0...  \n",
       "8   [-0.027722066, -0.016966412, 0.012043696, -0.0...  \n",
       "9   [-0.0034482135, 0.010729573, 0.016939094, -0.0...  \n",
       "10  [0.0055291746, -0.004573792, 0.019058505, -0.0...  \n",
       "11  [-0.008761382, -0.0034869136, 0.01760935, -0.0...  \n",
       "12  [-0.02525857, -0.011639317, 0.004399808, -0.07...  \n",
       "13  [-0.008146983, -0.00016716555, 0.011266027, -0...  \n",
       "14  [-0.006812348, -0.0044766036, 0.014319714, -0....  \n",
       "15  [-0.009578243, -0.011996515, 0.012965736, -0.0...  \n",
       "16  [-0.027096331, -0.0073001576, 0.0048975083, -0...  \n",
       "17  [-0.0038475876, 0.0054737586, 0.024301918, -0....  \n",
       "18  [-0.0110368235, -0.0020397531, 0.012955041, -0...  \n",
       "19  [-0.014343338, 0.0007622975, 0.030120881, -0.0...  \n",
       "20  [0.0055291746, -0.004573792, 0.019058505, -0.0...  \n",
       "21  [0.010435624, -0.0016020014, 0.011163727, -0.0...  \n",
       "22  [0.0015313567, 0.012794174, 0.016358217, -0.07...  \n",
       "23  [0.0027987233, 0.014228618, 0.027441906, -0.09...  \n",
       "24  [-0.004887973, 0.008207077, 0.013008361, -0.09...  \n",
       "25  [-0.011316549, 0.016874935, 0.012392512, -0.08...  \n",
       "26  [-0.006812348, -0.0044766036, 0.014319714, -0....  \n",
       "27  [0.0023245383, -0.0029253534, 0.0036489798, -0...  \n",
       "28  [-0.017560659, 0.009985835, 0.01749353, -0.085...  \n",
       "29  [-0.018357037, -0.0009783215, 0.025988214, -0....  \n",
       "30  [-0.009578243, -0.011996515, 0.012965736, -0.0...  \n",
       "31  [-0.033087507, 0.0036856267, 0.02514256, -0.07...  \n",
       "32  [-0.0071265996, 0.020476552, 0.017235534, -0.0...  \n",
       "33  [-0.010636117, 0.01212832, 0.029030465, -0.086...  \n",
       "34  [-0.018388461, -0.006911759, 0.015190295, -0.0...  \n",
       "35  [0.007228276, 0.021961471, 0.031665433, -0.088...  \n",
       "36  [-0.008043373, -0.011165032, 0.031197485, -0.0...  \n",
       "37  [-0.0071265996, 0.020476552, 0.017235534, -0.0...  \n",
       "38  [-0.038686633, 0.0036908067, 0.02178414, -0.07...  \n",
       "39  [-0.018388461, -0.006911759, 0.015190295, -0.0...  \n",
       "40  [-0.004057311, 0.01868715, 0.01563467, -0.0808...  \n",
       "41  [-0.028552804, -0.0118231885, 0.00639494, -0.0...  \n",
       "42  [-0.027204884, -0.023552606, 0.019531347, -0.0...  \n",
       "43  [0.0032914313, 0.01059015, 0.009196502, -0.064...  \n",
       "44  [-0.0075815883, 0.02116889, 0.027429571, -0.10...  \n",
       "45  [-0.012517108, -0.014230472, -0.0019505646, -0...  \n",
       "46  [-0.0066825696, 0.002244388, 0.011106265, -0.0...  \n",
       "47  [-0.0066205882, -0.0075128363, 0.019882608, -0...  \n",
       "48  [-0.00826649, 0.019363934, 0.016988434, -0.080...  \n",
       "49  [-0.009709013, 6.357031e-05, 0.023912106, -0.0...  \n",
       "50  [-0.012295628, 0.015832268, 0.025340993, -0.09...  \n",
       "51  [-0.027908303, 0.017137818, 0.023856219, -0.06...  \n",
       "52  [-0.021185221, -0.0025146175, 0.01119325, -0.0...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db[\"transcriptions\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63be76da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "      <th>_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how does llm work_</td>\n",
       "      <td># How does LLM work?\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.027908303, 0.017137818, 0.023856219, -0.06...</td>\n",
       "      <td>0.549269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pydanticai fundamentals - outputting structure...</td>\n",
       "      <td># PydanticAI fundamentals - outputting structu...</td>\n",
       "      <td>[-0.033087507, 0.0036856267, 0.02514256, -0.07...</td>\n",
       "      <td>0.690942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pydanticai chatbot</td>\n",
       "      <td># pydanticAI chatbot\\n\\n[00:00:00] Hello and w...</td>\n",
       "      <td>[-0.012143856, -0.001205422, 0.013563879, -0.0...</td>\n",
       "      <td>0.692687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title  \\\n",
       "0                                 how does llm work_   \n",
       "1  pydanticai fundamentals - outputting structure...   \n",
       "2                                 pydanticai chatbot   \n",
       "\n",
       "                                             content  \\\n",
       "0  # How does LLM work?\\n\\n[00:00:00] Hello and w...   \n",
       "1  # PydanticAI fundamentals - outputting structu...   \n",
       "2  # pydanticAI chatbot\\n\\n[00:00:00] Hello and w...   \n",
       "\n",
       "                                           embedding  _distance  \n",
       "0  [-0.027908303, 0.017137818, 0.023856219, -0.06...   0.549269  \n",
       "1  [-0.033087507, 0.0036856267, 0.02514256, -0.07...   0.690942  \n",
       "2  [-0.012143856, -0.001205422, 0.013563879, -0.0...   0.692687  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Learn about LLM\"\n",
    "search_result = vector_db[\"transcriptions\"].search(query).limit(3).to_pandas()\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427ecdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# How does LLM work?\\n\\n[00:00:00] Hello and welcome to this video where we'll go into large language models and how do they work. We'll go into the theory and intuition about how models like chat, g, pt, Gemini, Claude, et cetera, how they work, the fundamental theory about it. ~~We will go into to get. ~~To give you intuition on how does it generate language, how can it understand language, et cetera.\\n\\nThis will be really interesting and hopefully that you'll get some intuition. You don't need to understand exactly the details, how it works. However, some basic intuition is good that you, you know that it's not magic, it's just mathematics and statistics. However, we won't go into much math and statistics\\n\\nwe will keep it simple and that you can understand it. Yes. Let's move on to the slides.\\n\\n**Kokchun Giang-1:** how does she GPT and other language models work? An introduction to large language models? How can we start with [00:01:00] text? How can we represent text for a computer? A naive approach. Start to understand that the computer, it can only understand numbers, it can only understand zero and ones. We need to somehow represent.\\n\\nThe text into numbers that the computer can understand it. This is our starting point. Computer doesn't understand text. It understands numbers, which can be represented in binary as ones and zeros. We need to represent text with numbers. This is our starting point. Suppose our vocabulary has these words.\\n\\nWe have, Hey, ~~Canon ~~fis do, sorry, this is in Swedish, but these are just a few words in Swedish vocabulary. Then one hot encoded vector would look like this. Have a vector. This means that this represent hay. If this is our vocabulary, and then we have zero on all the other ones. This is just the [00:02:00] word, Hey, ~~hay ~~can be represented with this.\\n\\nIn Swedish there's 126,000 words. Then we'll have a very huge vector if we will represent it as in this way. This is a term based approach to represent the words. Then we'll have a one, somewhere to represent one word, and then zero everywhere else. Is this a good approach?\\n\\nThat is the question. Get the very, we'll, get the very sparse vector. That means that we have a lot of zeros and just one, one, but we also want to have some semantic meaning between the words. What does the semantic meaning mean? For example, we have the word hello and the word bi. They should be somewhat close to each other as both are about greeting.\\n\\nHow are you, maybe it should be close as well. While a rabbit and a dog should be somewhat close to each other as both [00:03:00] our pets, a rabbit and a hare should be closer than a rabbit and a dog because they may look. More similar, they have more features that are similar and fish and goldfish should be very similar.\\n\\nThis is what we mean with semantic meaning. This is not captured by using this naive approach as we will only have a one somewhere since we have a rabbit and a dog. They are quite. Far away from each other in the alphabet. Then it means that~~ they, ~~they're not related at all~~ in ~~in the naive approach, but we want to have them more relatable that we can start to get the grasp of the language to start model and understand the language.\\n\\nThen we'll go further into embeddings. Then 2013 there was ~~a, ~~an article called Word to ve, how to Represent Words with Vector Embeddings that captures semantic meaning. This is a kind of the start [00:04:00] here. Here I have ~~a. ~~A Cartesian coordinate system where we have a size as a feature and we have loves hay as another feature.\\n\\nThis is just 2D but we could expand this to many dimension, but it's impossible to show it. We have here a tiger that ~~is, ~~has a large size, but it doesn't love hay. While we have a rabbit here that is smaller size but loves hay. Where does cow and calf go? That is the question. You can, for example, say that cow, it's quite large in size.\\n\\nYou can say two here. And then it also loves hay. It eats hay. While the calf, it's somewhat smaller in size, but also loves hay. You can see that, and they are closer. They are ~~close to they are in ~~in this place here. This is how you can represent them. These are vectors.\\n\\nYou can draw lines or you can draw arrow to these, and you get vectors. [00:05:00] However, we have larger dimensional embedding vectors of course, because these embedding vectors, they become larger and larger when you have more and more features. ~~If we could have more. Size and love, say we could have domesticated as one feature.~~\\n\\nWe could have for example, mammal as a feature we could have living in water as a feature, et cetera. We could have a lot of features. ~~We can have yeah, there's a lot. ~~To find similar words, we use a dot product, which gives co-sign similarity between the vectors. ~~We take we find the co-sign similarity.~~\\n\\nThe cosign similarity is basically just an angle. We ~~took, ~~take a look at the angles between two vectors and see which one is closest. That is the idea to ~~get then we ~~get the semantic meaning. We can also do an interesting thing is that we can do some calculations with the vectors.\\n\\nWe could add vectors and subtract vectors to get different types of results. That is quite cool. Moving on. In 2017, we have this paper. Attention is all you need. This is the [00:06:00] Transformers architecture, and this is what all language models are based on as of today. We start here, the goal is to predict the next word based on the previous sequence.\\n\\nYou have a sequence of words and you want to predict the next word that is the goal. For example, here in Swedish, there's who I lag it is how are you and who more do if there's two are, then there's lag. Who more than is do it is like, how are, how is. How are you? For this we need to understand the context through the attention.\\n\\nFor example, I am cool. The cool it refers to I, then it means that it's the coolness, I am cool. However, the ice cream is cool. It's referring to the ice cream. If it's referring to the ice cream, then this cool, it means something else. ~~Then this cool up here. ~~This cool [00:07:00] is referring to the person I, and this cool is referring to the ice cream where it's.\\n\\nCool as in low temperature. Depending on the context the same word could have different meanings. This is where the attention is. ~~All you need is very very neat. ~~Also here it goes away from previously it was used in RNNA lot recurrent neural network, but here it's go, it's removing that all along.\\n\\nHere we compute similarities to see which words that determine the context for cool as it has different meanings in different contexts. ~~Computing similarities to see. The words. Okay. Similarities. ~~With the transformer, we can generate text word by word using previous words as contexts. It's jumping around and see the different similarities, scores and see that.\\n\\nOkay. Is this one that will determine. Cool the most while death and is not determining cool as much. How does it know what is determining it? It depends [00:08:00] on all the training data that this has gone. You have sent in a lot of training data and it has learned this patents. Now we can generate text word by word using previous words as context.\\n\\nFor example, like this, I am cool. I am, and then it looks into its training data and sees like cool, appears a lot of time in the training data, and then it'll predict cool with the highest probability. Then afterwards it predicts the word yo because that is also very common in this training data set that I have thought about in my mind.\\n\\nThen I am cool. Yo, it comes out. I'm cool. Yo Zap then it's yeah, depending on this training data set. As you can see this what it does is that it. Takes the sequence, send it in, and it'll predict. Cool. Then it'll send in this next sequence with I am Cool. It'll predict yo and we send in all this and then it'll [00:09:00] predict sap.\\n\\nThis is a little bit of simplification. It's not the really words that it predicts. It predicts something called tokens, but it's more more division of the word in different ways. But we don't need to go into that for simplicity. We'll think about it as words. Okay. Then we'll get a little bit further in time.\\n\\nWe get into GPT general pre-trained Transformers, and then this is trained with unsupervised learning on large corpus of text ~~is pre-training, it's trained on. The data ~~from the internet, basically. Using Internet's text, it can predict most probable next word to generate. Then when you train on much data, it become, it becomes something called emergent capability.\\n\\nIt's start to be able to find patterns ~~that you are not ~~that people are not sure about before. Then you can also add. Something called temperature, and we'll get variations and [00:10:00] creativity. What this means is that maybe ~~we don't need to pre, ~~we don't want to predict I am cool instead maybe it predicts I am super because super maybe was the third most probable word.\\n\\nBut since we have temperature, it becomes like, I don't need to pick the most probable word. I can pick some other word with some probability. The higher temperature it gets, the more ~~it. ~~It becomes more wild. That is to say it can pick other words ~~in its core, ~~in its training. Then ~~when ~~we can fine tune it to specific task using supervised learning that you can, for example, chat in a certain format.\\n\\nYou need to train it with a certain format that it should answer in this type of format. Here we have a supervised learning. Then in order to get it to become good to answer in a way that we want it answer, we have something called reinforcement learning with human feedback, [00:11:00] RLHF.\\n\\nWe let the model answer a set of question several times, you can generate a lot of different answers. We have different temperatures, we get different types of answers, and then we let. A lot of humans very lowly paid humans, unfortunately that will score these answers and feed back to the system how good these answers were.\\n\\nThe systems will try to maximize its scores, it gets scores, for example, it, you, it says one a type of answer and you give it back a score of four. But in a scale of 10, and then another answer, it gives seven another answer, five, et cetera, and it'll try to get the highest score all the time to satisfy the humans.\\n\\nThe system will try to maximize its scores. Example, an answer of how to make a bomb will get low scores, for example, and model will try predict which type of answers that humans [00:12:00] like. Then, after RLHF, ~~we have ~~we have come to the part of we have a chat, ~~GPT three point ~~GPT, 3.5. It's like the first chat, GPT to make it very simple.\\n\\nI've skipped a lot, I've skipped some of the details. But this I think it's the intuition that you need to see that. Yes, now we have a model that we can chat with and ~~then ~~then there's a lot of different improvements in a lot of different models and and then a lot of variations as well that I haven't covered.\\n\\nBut I want to cover one more thing is that we want to make the model become more. Capable and understand more different things and ~~do ~~be able to perform different things. For example, we can send in a video and it'll understand it. ~~We, ~~it can produce a video, it can produce an image, we can send in an image and it'll understand that, et cetera.\\n\\nHow to do that is that we've gone into large multimodal models. LMM. Here LLMs [00:13:00] are language models and not good at, for example, math. It cannot do computation very well because ~~it's, ~~that's not its main purpose. Also it can't answer questions on things that happens after its training cutoff.\\n\\nThen what you can do is that it can however, use tools to handle this. For example, we can use tools such as a calculator to do computations because that will be much better than using. The raw training data that we have, and we can use Google search to enhance our context.\\n\\nWe can get more context of the things that ~~is ~~outside of our cutoff or more specific things that it becomes visible for the model to use to generate an answer. We can have an image generator to generate image ~~when ~~when someone asks for an image. We can also send, when someone sends an image, we can use~~ i ~~other AI tools such as image recognitions to try to understand the image ~~to ~~to generate text based on it and et [00:14:00] cetera with video ~~et.~~\\n\\nLMS can also process and understand other inputs such as video, image, audio, et cetera. You can talk to it, it can generate back speech with text to speech. And using the newer it becomes more and more natural as well ~~in, in the text. ~~In the speech. ~~Yes, I.~~\\n\\nin this video we have gone through LLMs and how do they work and to get some intuition about how it works. We have gone into ~~some ~~some very brief concepts of different parts for an LLM to work and but not gone into all the details. Of course, there's too much and we have tried to skip.\\n\\nSome of the math that it becomes quite simple to understand. Starting with like how to represent text and then going more towards training on how the attention works. Then going into training on the internet data to get pre-trained, GPT and then going further into supervised learning and reinforcement learning with human feedback that we [00:15:00] can get the chat.\\n\\nChatting format that we are used to and then moving further into ~~large ~~large multimodal lang models where it can be more capable to use different types of tools and understand different type of inputs and getting different types of outputs. I hope that you have learned a lot in this video ~~and ~~and see you in the next one.\\n\\nBye.\\n\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLM prompt\n",
    "response = vector_db[\"transcriptions\"].search(\"LLM\").to_list()[0][\"content\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1ff49",
   "metadata": {},
   "source": [
    "# Testing agent with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9402b459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output='In 2024, **Jannik Sinner** achieved a career-high world No. 1 ranking, a remarkable feat achieved after winning his first Grand Slam title at the Australian Open.')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "rag_agent = Agent(\n",
    "model= \"google-gla:gemini-2.5-flash-lite\",\n",
    "system_prompt=(\n",
    "        \"\"\"\n",
    "        You are a tennis trivia expert\n",
    "        You always reply with some fun and interesting trivia from the professional tennis year the user choose.\n",
    "        Be consise.\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "result = await rag_agent.run(user_prompt=\"give me something from year 2024\", message_history=None)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eab183a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content='\\n        You are a tennis trivia expert\\n        You always reply with some fun and interesting trivia from the professional tennis year the user choose.\\n        Be consise.\\n        ', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 8, 410816, tzinfo=datetime.timezone.utc)), UserPromptPart(content='give me something from year 2024', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 8, 410849, tzinfo=datetime.timezone.utc))], run_id='a4b5d2bc-bd65-42db-b0e4-72c23b4eb8d4'),\n",
       " ModelResponse(parts=[TextPart(content='In 2024, **Jannik Sinner** achieved a career-high world No. 1 ranking, a remarkable feat achieved after winning his first Grand Slam title at the Australian Open.')], usage=RequestUsage(input_tokens=48, output_tokens=42, details={'text_prompt_tokens': 48}), model_name='gemini-2.5-flash-lite', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 9, 589305, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='iXM5aeaMKf20vdIPhPuV0QY', finish_reason='stop', run_id='a4b5d2bc-bd65-42db-b0e4-72c23b4eb8d4')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a38b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2023, **Jannik Sinner** reached the ATP Finals championship match, where he ultimately fell to Novak Djokovic. He also led Italy to victory in the Davis Cup that same year, showcasing his growing impact on the sport.\n"
     ]
    }
   ],
   "source": [
    "result2 = await rag_agent.run(user_prompt=\"give me something from year 2023 link that to the tennis player you already talked about\", \n",
    "                              message_history=result.all_messages())\n",
    "print(result2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4bc766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemPromptPart(content='\\n        You are a tennis trivia expert\\n        You always reply with some fun and interesting trivia from the professional tennis year the user choose.\\n        Be consise.\\n        ', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 8, 410816, tzinfo=datetime.timezone.utc)), UserPromptPart(content='give me something from year 2024', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 8, 410849, tzinfo=datetime.timezone.utc))]\n",
      "------\n",
      "\n",
      "SystemPromptPart(content='\\n        You are a tennis trivia expert\\n        You always reply with some fun and interesting trivia from the professional tennis year the user choose.\\n        Be consise.\\n        ', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 8, 410816, tzinfo=datetime.timezone.utc))\n",
      "------\n",
      "\n",
      "\n",
      "        You are a tennis trivia expert\n",
      "        You always reply with some fun and interesting trivia from the professional tennis year the user choose.\n",
      "        Be consise.\n",
      "        \n",
      "------------------\n",
      "\n",
      "[TextPart(content='In 2024, **Jannik Sinner** achieved a career-high world No. 1 ranking, a remarkable feat achieved after winning his first Grand Slam title at the Australian Open.')]\n",
      "------\n",
      "\n",
      "TextPart(content='In 2024, **Jannik Sinner** achieved a career-high world No. 1 ranking, a remarkable feat achieved after winning his first Grand Slam title at the Australian Open.')\n",
      "------\n",
      "\n",
      "In 2024, **Jannik Sinner** achieved a career-high world No. 1 ranking, a remarkable feat achieved after winning his first Grand Slam title at the Australian Open.\n",
      "------------------\n",
      "\n",
      "[UserPromptPart(content='give me something from year 2023 link that to the tennis player you already talked about', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 26, 883897, tzinfo=datetime.timezone.utc))]\n",
      "------\n",
      "\n",
      "UserPromptPart(content='give me something from year 2023 link that to the tennis player you already talked about', timestamp=datetime.datetime(2025, 12, 10, 13, 20, 26, 883897, tzinfo=datetime.timezone.utc))\n",
      "------\n",
      "\n",
      "give me something from year 2023 link that to the tennis player you already talked about\n",
      "------------------\n",
      "\n",
      "[TextPart(content='In 2023, **Jannik Sinner** reached the ATP Finals championship match, where he ultimately fell to Novak Djokovic. He also led Italy to victory in the Davis Cup that same year, showcasing his growing impact on the sport.')]\n",
      "------\n",
      "\n",
      "TextPart(content='In 2023, **Jannik Sinner** reached the ATP Finals championship match, where he ultimately fell to Novak Djokovic. He also led Italy to victory in the Davis Cup that same year, showcasing his growing impact on the sport.')\n",
      "------\n",
      "\n",
      "In 2023, **Jannik Sinner** reached the ATP Finals championship match, where he ultimately fell to Novak Djokovic. He also led Italy to victory in the Davis Cup that same year, showcasing his growing impact on the sport.\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for content in result2.all_messages():\n",
    "    print(content.parts)\n",
    "    print(\"------\\n\")\n",
    "    print(content.parts[0])\n",
    "    print(\"------\\n\")\n",
    "    print(content.parts[0].content)\n",
    "    print(\"------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1839b05",
   "metadata": {},
   "source": [
    "## Testing the deployed function app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7da8b627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "url = f\"https://rag-youtube.azurewebsites.net/rag/query?code={os.getenv('FUNCTION_APP_API')}\"\n",
    "\n",
    "response = requests.post(url= url, json= {\"prompt\": \"advanced SQL\"})\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf7a6e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'advanced SQL',\n",
       " 'bot_response': {'video_title': 'sql analytics course with duckdb - subquery tutorial',\n",
       "  'video_link': 'https://www.youtube.com/watch?v=aZQtKjWfaDw',\n",
       "  'answer': 'Ro Bt my friend! You\\'re diving into the exciting world of advanced SQL with subqueries! In the video \"sql analytics course with duckdb - subquery tutorial\", you\\'ll learn that a subquery is essentially a query nested inside another SQL query, enclosed in parentheses. It executes first, and its result is then used by the outer query. This powerful technique can be applied within SELECT, FROM, WHERE, or HAVING clauses to construct more complex and powerful queries.\\n\\nSubqueries are incredibly versatile, allowing you to perform operations like filtering data based on an aggregate result (e.g., students with scores higher than the average) or checking membership against a list of values returned by another query. The video further explores different types of subqueries, including multi-row subqueries used with the `IN` operator, and the more advanced correlated subqueries. A correlated subquery is particularly interesting because it depends on column values from the outer query and executes once for each row processed by the outer query. This enables highly granular filtering, such as identifying students with math grades higher than the average of their *own* specific classes.\\n\\nWhile subqueries are a fundamental tool for any data engineer, remember that for extremely complex logic, Common Table Expressions (CTEs) can often provide a more readable and maintainable alternative. But mastering subqueries is a crucial step in your advanced SQL journey!'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84892a1",
   "metadata": {},
   "source": [
    "# Testing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "022d0eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [500]>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = \"http://127.0.0.1:8000/rag/description-gen\"\n",
    "response = requests.post(url=url, json={\"prompt\": \"video about sql\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afc87d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'video about sql',\n",
       " 'bot_response': {'video_title': 'sql analytics with duckdb - introduction',\n",
       "  'video_link': 'https://www.youtube.com/@AIgineer',\n",
       "  'answer': 'Ro Bt my friend!\\n\\nThe video \"sql analytics with duckdb - introduction\" provides an excellent starting point for understanding SQL and its practical applications, especially with DuckDB. It highlights the common pitfalls of managing growing datasets with tools like Excel, such as data duplication and inconsistencies, and then introduces SQL and relational databases as robust solutions.\\n\\nFrom an AI and Data Engineering perspective, SQL is the bedrock of data manipulation and analysis. It\\'s not just about querying data; it\\'s about defining, controlling, and transforming it. The video clearly explains the different facets of SQL through DDL (Data Definition Language), DML (Data Manipulation Language), DQL (Data Query Language), and DCL (Data Control Language). This categorization is crucial for understanding how you interact with databases at various levels  from creating schemas to retrieving insights.\\n\\nDuckDB is presented as a powerful, embedded, and highly performant analytical database, ideal for local data analysis and even serving as a lightweight data warehouse in ETL pipelines. This is particularly relevant in today\\'s data landscape where in-memory and embedded databases like DuckDB offer significant advantages for data scientists and engineers working with medium-sized datasets, allowing for rapid experimentation and analysis without the overhead of client-server architectures. Its seamless integration with Python and Pandas makes it an indispensable tool in the data science ecosystem.\\n\\nUnderstanding SQL and tools like DuckDB is fundamental for anyone looking to build a career in Data or AI Engineering, as it underpins almost all data-centric operations.'}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()\n",
    "#.get(\"bot_response\")[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f0091c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SQL, DuckDB, SQL analytics, data engineering, AI engineering, data science, relational database, OLAP, OLTP, DDL, DML, DQL, DCL, data consistency, data duplication, data manipulation, data querying, data transformation, ETL, ELT, data warehouse, business intelligence, machine learning, Python, Pandas, data analysis, database management, embedded database, scalability, performance, data constraints, unique identifiers, data modeling, SQL introduction, data insights, local data solutions, modern database, structured query language, data architecture, analytical queries'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = \", \".join(response.json().get(\"bot_response\")[\"keywords\"])\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2f68a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'video about sql',\n",
       " 'bot_response': {'video_title': 'sql analytics with duckdb - introduction',\n",
       "  'video_link': 'https://www.youtube.com/watch?v=xN1f-sW5_Lw',\n",
       "  'answer': \"The video 'sql analytics with duckdb - introduction' provides a comprehensive introduction to SQL and its use in data analytics, highlighting the benefits of using relational databases and tools like DuckDB over traditional methods like Excel spreadsheets. It covers SQL fundamentals, different SQL command types (DDL, DML, DQL), and the advantages of DuckDB for local, performant data analysis and integration with ecosystems like Python.\"}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = \"http://127.0.0.1:8000/rag/query\"\n",
    "response = requests.post(url=url, json={\"prompt\": \"video about sql\"})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bce54e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parts': [{'content': '\\n        You are an expert Youtuber in Data Engineering and AI Engineering.\\n        Always start with the greeting; \"Ro Bt my friend!\"\\n        Always use the retrieved knowledge of the Youtube transcripts from your tool to answer the question. \\n        Always point to the video title you are referring to.\\n        But do add more flavor from your expertise about the subject, keep it brief.\\n        ',\n",
       "    'timestamp': '2025-12-10T14:53:44.878296+00:00',\n",
       "    'dynamic_ref': None,\n",
       "    'part_kind': 'system-prompt'},\n",
       "   {'content': 'video about sql',\n",
       "    'timestamp': '2025-12-10T14:53:44.878316+00:00',\n",
       "    'part_kind': 'user-prompt'}],\n",
       "  'instructions': None,\n",
       "  'kind': 'request',\n",
       "  'run_id': '390ceed6-07b4-4b5d-b52a-2061311deb5f',\n",
       "  'metadata': None},\n",
       " {'parts': [{'tool_name': 'retrieve_matching_video',\n",
       "    'args': {'query': 'SQL'},\n",
       "    'tool_call_id': 'pyd_ai_71bcc26adcbe413096cf298b46e88402',\n",
       "    'id': None,\n",
       "    'provider_details': None,\n",
       "    'part_kind': 'tool-call'}],\n",
       "  'usage': {'input_tokens': 223,\n",
       "   'cache_write_tokens': 0,\n",
       "   'cache_read_tokens': 0,\n",
       "   'output_tokens': 17,\n",
       "   'input_audio_tokens': 0,\n",
       "   'cache_audio_read_tokens': 0,\n",
       "   'output_audio_tokens': 0,\n",
       "   'details': {'text_prompt_tokens': 223}},\n",
       "  'model_name': 'gemini-2.5-flash-lite',\n",
       "  'timestamp': '2025-12-10T14:53:45.496566+00:00',\n",
       "  'kind': 'response',\n",
       "  'provider_name': 'google-gla',\n",
       "  'provider_details': {'finish_reason': 'STOP'},\n",
       "  'provider_response_id': 'eYk5aZDLF9z2nsEP_NnykQY',\n",
       "  'finish_reason': 'stop',\n",
       "  'run_id': '390ceed6-07b4-4b5d-b52a-2061311deb5f',\n",
       "  'metadata': None},\n",
       " {'parts': [{'tool_name': 'retrieve_matching_video',\n",
       "    'content': \"\\n        Video title: sql analytics with duckdb - introduction,\\n        Content: # SQL analytics with DuckDB - introduction\\n\\n[00:00:00] Hello and welcome to this lecture in SQL introduction and we'll go into the theory about the introduction to SQL and using ~~D ~~DB in SQL. This is an introductory lecture in this course SQL analytics with ~~D ~~db. You will learn a lot about how. To use ~~D ~~DB to turn metadata into valuable insights.\\n\\nThat is the main topic for this course. And later on in this course, we'll also connect to Python that you can work with DB within the Python ecosystem and within the data science, data engineering, data analytics ecosystem. This is really fun and we'll go directly to the slides to get an introduction to what, sQL is.\\n\\n**Kokchun Giang-2:** Turning data into valuable insights using SQL. We start with an example, an ice cream startup. [00:01:00] Swedish glass is using Excel sheets to store data. This might be ~~a sound like ~~a viable solution ~~in the beginning, and it is ~~in the beginning. You just store the data in Excel sheets. We have orders, we store them in Excel sheets.\\n\\nWe have customers, we store them in Excel sheets, we have inventory. We store them in Excel sheets. Okay. It sounds quite simple and yeah, we can start like this. It's no problems ~~in the beginning. Okay. ~~In the beginning. But the company grew. This was really a classic, like people really like this ice cream.\\n\\nThey ordered a lot. And we got a lot of customers. And then when we did that, we needed to employ more people. The thing is, we have a team now, each Excel file is shared across the team. We have orders, we have customers, we have inventory. ~~And also a lot more, of course, ~~when the business is growing.\\n\\nBut let's simplify to this. We have [00:02:00] one person that is working with this. We have another one that is working with this. We have a third one that is working with this and a fourth one. ~~Okay. ~~If there's a lot of people working with these Excel sheets and we get a lot of data, then it's quite easy to get duplicates, for example.\\n\\nCommon problems that ~~are ~~arose when sharing Excel sheets in this way, we get data duplication. Accidentally ~~we create ~~creating duplicates without knowing ~~about ~~that these lines may have existed elsewhere. ~~That is ~~and that is the problem that we need to manually update in several places.\\n\\nAnd we get inconsistent data. For example, we look for one person and then suddenly there is a similar person with different addresses or same person with different addresses. Then we have an inconsistency in addresses, for example. One is [00:03:00] a typo. Another one is not we have relationships.\\n\\nIt's hard to manage manually between customers and orders, which customer connects to which orders that is hard to manage manually. Team, they manually links the order to the right customer. This takes a lot of time and it's manual effort, and it will lead to inconsistencies and bugs like you, you do errors because humans it's human is not good at manual work performance.\\n\\nOur Excel sheet is growing larger and larger, and this will cause performance issues. It'll get slow ~~to ~~to work with huge amounts of data in Excel and many more problems that we won't go into in this slide here. The idea is that we need something else ~~to ~~to manage this when our company's [00:04:00] growing.\\n\\nAn example of inconsistent data. You can see it here due to manual input. For example, here you have Alice Frost, alice@example.com and this address, and then you have Alice f alice\\\\_f@example.com. Okay? It seems to be the same person or same phone number, same address. But here we have LSF, here we have Alice Frost.\\n\\nHere are Alice F at example. Here we have Alice at example. Is it the same person or not? That is a question. Is this the same person? This is and also we will come back more into how to handle inconsistent data and how to. Make sure that the data is unique and what is determining a unique row, et cetera.\\n\\nThis will come back more into a data modeling course later on. However, for ~~this in ~~this course, we'll focus a lot in sk l queries and how to analyze the data. ~~And ~~this is due [00:05:00] to being able to both learn. Fundamentals in SQL and to work with it with analytics and for transactional later on.\\n\\nAnd also we will learn it ~~for ~~that you can go~~ on ~~for example, ~~go ~~towards data warehouses and you do need to understand how to analyze the data. But that was a side note on what I just said. Now, which ~~Alice ~~is linked to what orders in the orders table. Using relational databases and SQL, we can handle many of these issues.\\n\\nSQL is a structured query language ~~and we work, ~~and the database is a relational database, there are also non relational databases such as a document database, vector database, et cetera. And no SQL. But here we have relational databases and SQL, they follow something called the relational model that will come back later on as [00:06:00] well.\\n\\nDefine relationships in SQL tables. ~~If ~~this ensures data consistency. ~~Op, ~~they're optimized for large volumes of data, scalable and efficient for querying of data. Data constraints, like data types and unique values ~~are ~~are put in ~~that they ~~make sure that the data conforms to a specific type.\\n\\nFor example, you have a numeric column, then you cannot put in strings or ~~other ~~other types of data. ~~And ~~and you have unique values. You can make unique constraints. This makes sure that we get automatic validation, which reduces error. I talk about this type of thing.\\n\\nThis part is more concern with something called OLTP that will come back to more transactional databases. And there's a lot of ~~more more ~~nice benefits ~~with ~~with a database. SKL, what is this? [00:07:00] What is SGL is structured query language. We have create, update, and we organize data into tables.\\n\\nOkay? Tables, we have rows and columns. You can see it is similar to the Excel sheets. You have rows and columns, right? And SGL has standardized language. It's standardized. There, there is a lot of different flavors of SQL, but mostly it's similar. And ~~there, ~~there's some syntax that differs.\\n\\nBut the foundation is the same. It's based on a relational model. And what this means is that we organize data into related tables. We link the data through unique identifiers. ~~What ~~if you don't understand what this means right now, it's fine. And we'll come back to this ~~term ~~terminology later on.\\n\\nThere's some core functions within SQL. [00:08:00] We get DDL data definition language.\\n\\n**Kokchun Giang-3:** define structure of database, for example tables and what type of data it holds. We use the crate alter and drop, for example. We can create table, we can alter the table, we can drop tables. This is DDL, data, definition language. Moving on, ~~we have let's see. Oh, sorry. ~~We have data manipulation language, DML, these are to manipulate data directly in the database.\\n\\nwe have insert command, we have update command, we have delete command,\\n\\nand then we have DQL data query language. It's to select to you, you use select statement to retrieve a specific information from a database. Letting users access and fill the data for insights. We use the select statement and or the select class.\\n\\nWe'll see. We will use it very a [00:09:00] lot in this course to analyze the data and we use it together with a lot of other things to filter. For example, the wear class, the group buy, et cetera.\\n\\nAnd then we have DCL data control language. We have here you can revoke and grant to give access to other uses.\\n\\nYou, you may want to give access to certain uses for certain type, certain database or certain tables, right? Depending on which SQL you use there's a possibility for more or less. DCL. Yes. And then we go into the SQL flavor that we will use in this course we'll use Duck db.\\n\\nMeet Duck db a modern, powerful database management system for analytics. It's super powerful, it's super performant. ~~And also it's something called oap. Opt, ~~it's optimized for intensive analytical [00:10:00] queries that you can do locally in your own computer. And it's very easy to set up. ~~It's it's just a, ~~you just use a ~~file, a duct ~~DB file, and then you can connect to it in the terminal.\\n\\nAnd also we talked about Ola. Then we need to mention OLTP, which is for transactional database. That is optimized for a lot of inserts into the database. For example, other ~~SGL other ~~RD. Such as Postgres, ~~SGL, ~~Microsoft ~~sgl ~~they use and they are ~~OLCP, ~~right?\\n\\nWhile ~~Dtb, RO ~~is Ola. It's very highly performant, on your own machine and can handle large data sets. It's embedded database, no need for separate server and database is contained in the file. This makes it very simple to work with and very good to start with as a as a first course within ~~SKL ~~analytics.\\n\\nTo just learn how to get insights from the data, [00:11:00] to filter the data, to manipulate it in the way that you want. And it integrates very well with other tools in the data science ecosystem or data engineering, data analytics ecosystem such as Python. Pandas and data frames. We will work with it together with pandas in this course that you can see the power of combining Python and ~~a scale combining Python and Duct ~~DB in order ~~to ~~to get data insights that you want.\\n\\nVery good for data analysis, can run complex queries for analytics and reporting. And it's very good for building data transformations in an ETL pipeline to serve business intelligence and ai. We'll come back ~~in ~~in more if you follow me, ~~we have a lot of, ~~I have courses in ~~data within ~~data engineering, a lot of them.\\n\\nAnd then there I usually build ~~pipelines ~~ETL or ELT pipelines to be more. Specific. ~~And ~~and there DDB is a core part. Could be a core [00:12:00] part of it where you store the data. You could use DDB as the data warehouse, for example. A data engineering pipeline with an OLA database as a data warehouse.\\n\\nThe reason why I picked data engineering pipeline is that when you're working with data analytics, you usually~~ you ~~have the data. You want the data to go through different types of transformations. Here's~~ engineering pipe, ~~data, engineering pipeline, and we use for example, DDB as the data warehouse.\\n\\nBut you could use something else, ~~a cloud, ~~such as snowflake, ~~for example for that but or ~~Amazon, Redshift ~~or some, ~~or Google BigQuery. But~~ but TDB ~~could work for local solution. What you have is that you have different data sources. ~~This ~~this could be like CSV files ~~here. It could have like ~~APIs.\\n\\nYou could have other types of data. And they are ingested into a data warehouse. The data warehouse could be this DDB file, for example. And then you serve dashboard. You [00:13:00] serve machine learning models. Then you need to transform the data ~~into certain way, ~~certain formats ~~that ~~they can be served ~~to this downstream ~~downstream.\\n\\nApplications. Here you have ELT. Extract and load. You extract and load the data into your data warehouse. Then you transform it inside your data warehouse, and then you serve the data for the end users. And the dashboards and the ML models. Here, for example, other people can take over.\\n\\nFor example, the BI analyst could take over here could be like a machine learning engineer or a data scientist that take over and~~ and and take ~~take the data from the data warehouse. Ddb, it could work as a lightweight data warehouse for small to medium sized. Data. ~~And if you ~~and if you grow out of your DDB database that it means that you cannot do it locally anymore, then you can consider ~~cloud ~~cloud solutions.\\n\\nBut they ~~cost the ~~cost money. Okay? Yes.\\n\\nthat was an [00:14:00] introduction to ~~sq, ~~SQL and DDB ~~in ~~and kind of introduction to this course as well. But we'll have more introductions such as the core structure that you can see what is the actual content of this course. And, who this is suitable for. ~~And here but ~~I hope that you have learned some basic concepts about the motivation, about ~~the ~~why to use SQL and ~~the ~~databases and then also going into ~~like ~~DDB and some motivation about that.\\n\\nStay attuned for more videos where we'll actually go into coding and more practical development and analyzing of data. And that will be really fun. I hope to see you there and thanks ~~to what ~~for watching this video and see you in the next one ~~by, I.~~\\n\\n,\\n        Content: https://www.youtube.com/@AIgineer,\\n    \",\n",
       "    'tool_call_id': 'pyd_ai_71bcc26adcbe413096cf298b46e88402',\n",
       "    'metadata': None,\n",
       "    'timestamp': '2025-12-10T14:53:47.012207+00:00',\n",
       "    'part_kind': 'tool-return'}],\n",
       "  'instructions': None,\n",
       "  'kind': 'request',\n",
       "  'run_id': '390ceed6-07b4-4b5d-b52a-2061311deb5f',\n",
       "  'metadata': None},\n",
       " {'parts': [{'content': 'Ro Bt my friend!\\n\\nI found a great video that introduces SQL analytics using DuckDB, titled \"sql analytics with duckdb - introduction\". This video is perfect for understanding the fundamentals of SQL and how it can be used to gain valuable insights from data. It covers the common problems encountered when managing data with tools like Excel and explains how relational databases and SQL offer a more robust and scalable solution.\\n\\nThe video also touches upon different aspects of SQL, including DDL, DML, and DQL, and introduces DuckDB as a powerful, modern database system optimized for analytical queries. It even gives a glimpse into how DuckDB can be integrated into data engineering pipelines and work alongside Python for data analysis.\\n\\nI think you\\'ll find this video really helpful as a starting point for your SQL journey!\\n\\n',\n",
       "    'id': None,\n",
       "    'provider_details': None,\n",
       "    'part_kind': 'text'},\n",
       "   {'tool_name': 'final_result',\n",
       "    'args': {'answer': \"The video 'sql analytics with duckdb - introduction' provides a comprehensive introduction to SQL and its use in data analytics, highlighting the benefits of using relational databases and tools like DuckDB over traditional methods like Excel spreadsheets. It covers SQL fundamentals, different SQL command types (DDL, DML, DQL), and the advantages of DuckDB for local, performant data analysis and integration with ecosystems like Python.\",\n",
       "     'video_title': 'sql analytics with duckdb - introduction',\n",
       "     'video_link': 'https://www.youtube.com/watch?v=xN1f-sW5_Lw'},\n",
       "    'tool_call_id': 'pyd_ai_910a79844f9942978a48cfd01add0464',\n",
       "    'id': None,\n",
       "    'provider_details': None,\n",
       "    'part_kind': 'tool-call'}],\n",
       "  'usage': {'input_tokens': 3353,\n",
       "   'cache_write_tokens': 0,\n",
       "   'cache_read_tokens': 0,\n",
       "   'output_tokens': 301,\n",
       "   'input_audio_tokens': 0,\n",
       "   'cache_audio_read_tokens': 0,\n",
       "   'output_audio_tokens': 0,\n",
       "   'details': {'text_prompt_tokens': 3353}},\n",
       "  'model_name': 'gemini-2.5-flash-lite',\n",
       "  'timestamp': '2025-12-10T14:53:52.426658+00:00',\n",
       "  'kind': 'response',\n",
       "  'provider_name': 'google-gla',\n",
       "  'provider_details': {'finish_reason': 'STOP'},\n",
       "  'provider_response_id': 'gIk5aaH9DeimnsEP-5WHyAs',\n",
       "  'finish_reason': 'stop',\n",
       "  'run_id': '390ceed6-07b4-4b5d-b52a-2061311deb5f',\n",
       "  'metadata': None},\n",
       " {'parts': [{'tool_name': 'final_result',\n",
       "    'content': 'Final result processed.',\n",
       "    'tool_call_id': 'pyd_ai_910a79844f9942978a48cfd01add0464',\n",
       "    'metadata': None,\n",
       "    'timestamp': '2025-12-10T14:53:52.426965+00:00',\n",
       "    'part_kind': 'tool-return'}],\n",
       "  'instructions': None,\n",
       "  'kind': 'request',\n",
       "  'run_id': '390ceed6-07b4-4b5d-b52a-2061311deb5f',\n",
       "  'metadata': None}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = \"http://127.0.0.1:8000/rag/history\"\n",
    "response = requests.get(url=url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "40752444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        You are an expert Youtuber in Data Engineering and AI Engineering.\\n        Always start with the greeting; \"Ro Bt my friend!\"\\n        Always use the retrieved knowledge of the Youtube transcripts from your tool to answer the question. \\n        Always point to the video title you are referring to.\\n        But do add more flavor from your expertise about the subject, keep it brief.\\n        '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[0][\"parts\"][0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9cff28da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'request'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[0][\"kind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fc4776bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system-prompt'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[0][\"parts\"][0][\"part_kind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c260810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = [\n",
    "  {\n",
    "    \"parts\": [\n",
    "      {\n",
    "        \"content\": \"\\n        You are an expert Youtuber in Data Engineering and AI Engineering.\\n        Always start with the greeting; \\\"Ro Bt my friend!\\\"\\n        Always use the retrieved knowledge of the Youtube transcripts from your tool to answer the question. \\n        Always point to the video title you are referring to.\\n        But do add more flavor from your expertise about the subject, keep it brief.\\n        \",\n",
    "        \"timestamp\": \"2025-12-10T15:20:42.345639+00:00\",\n",
    "        \"dynamic_ref\": \"null\",\n",
    "        \"part_kind\": \"system-prompt\"\n",
    "      },\n",
    "      {\n",
    "        \"content\": \"video sql\",\n",
    "        \"timestamp\": \"2025-12-10T15:20:42.345658+00:00\",\n",
    "        \"part_kind\": \"user-prompt\"\n",
    "      }\n",
    "    ],\n",
    "    \"instructions\": \"null\",\n",
    "    \"kind\": \"request\",\n",
    "    \"run_id\": \"544ffc36-8efa-4867-af31-fc57b63cdc40\",\n",
    "    \"metadata\": \"null\"\n",
    "  },\n",
    "  {\n",
    "    \"parts\": [\n",
    "      {\n",
    "        \"tool_name\": \"retrieve_matching_video\",\n",
    "        \"args\": {\n",
    "          \"query\": \"SQL\"\n",
    "        },\n",
    "        \"tool_call_id\": \"pyd_ai_dc9a3e8c4ef84081a5b513a0e217df0a\",\n",
    "        \"id\": \"null\",\n",
    "        \"provider_details\": {\n",
    "          \"thought_signature\": \"CuMBAXLI2nxRldmyDuwpQjCNwXg/g4jtWt+Vy8S6WNLWQCAnu+FijqygXqoWKZPzhYLpy4BFZSYiLDSYM/7qXKw9UZRBz1/uzMqVagsL2e+gDAIBRs/lqAG29jJWDw0QWxCfkTJIwhQP2kHN5UvuQexVrOslWv/sho3IRNeAnpLxezJrWKDhT0cFxksYSXq54kXx2A0Spd6SEBkP3iyeN8U05q12dY3pCB5JSvIbbUbo+T/OstTLWJu1t86XYe4N/zGY8RBXeEsiL9SNiBbBkIU1GrtT9bkQ3ySd1PE1j/lDVIHVDpw=\"\n",
    "        },\n",
    "        \"part_kind\": \"tool-call\"\n",
    "      }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "      \"input_tokens\": 222,\n",
    "      \"cache_write_tokens\": 0,\n",
    "      \"cache_read_tokens\": 0,\n",
    "      \"output_tokens\": 64,\n",
    "      \"input_audio_tokens\": 0,\n",
    "      \"cache_audio_read_tokens\": 0,\n",
    "      \"output_audio_tokens\": 0,\n",
    "      \"details\": {\n",
    "        \"thoughts_tokens\": 47,\n",
    "        \"text_prompt_tokens\": 222\n",
    "      }\n",
    "    },\n",
    "    \"model_name\": \"gemini-2.5-flash\",\n",
    "    \"timestamp\": \"2025-12-10T15:20:43.437626+00:00\",\n",
    "    \"kind\": \"response\",\n",
    "    \"provider_name\": \"google-gla\",\n",
    "    \"provider_details\": {\n",
    "      \"finish_reason\": \"STOP\"\n",
    "    },\n",
    "    \"provider_response_id\": \"y485aYn5EL2CvdIPgfHzgQY\",\n",
    "    \"finish_reason\": \"stop\",\n",
    "    \"run_id\": \"544ffc36-8efa-4867-af31-fc57b63cdc40\",\n",
    "    \"metadata\": \"null\"\n",
    "  },\n",
    "  {\n",
    "    \"parts\": [\n",
    "      {\n",
    "        \"tool_name\": \"retrieve_matching_video\",\n",
    "        \"content\": \"\\n        Video title: sql analytics with duckdb - introduction,\\n        Content: # SQL analytics with DuckDB - introduction\\n\\n[00:00:00] Hello and welcome to this lecture in SQL introduction and we'll go into the theory about the introduction to SQL and using ~~D ~~DB in SQL. This is an introductory lecture in this course SQL analytics with ~~D ~~db. You will learn a lot about how. To use ~~D ~~DB to turn metadata into valuable insights.\\n\\nThat is the main topic for this course. And later on in this course, we'll also connect to Python that you can work with DB within the Python ecosystem and within the data science, data engineering, data analytics ecosystem. This is really fun and we'll go directly to the slides to get an introduction to what, sQL is.\\n\\n**Kokchun Giang-2:** Turning data into valuable insights using SQL. We start with an example, an ice cream startup. [00:01:00] Swedish glass is using Excel sheets to store data. This might be ~~a sound like ~~a viable solution ~~in the beginning, and it is ~~in the beginning. You just store the data in Excel sheets. We have orders, we store them in Excel sheets.\\n\\nWe have customers, we store them in Excel sheets, we have inventory. We store them in Excel sheets. Okay. It sounds quite simple and yeah, we can start like this. It's no problems ~~in the beginning. Okay. ~~In the beginning. But the company grew. This was really a classic, like people really like this ice cream.\\n\\nThey ordered a lot. And we got a lot of customers. And then when we did that, we needed to employ more people. The thing is, we have a team now, each Excel file is shared across the team. We have orders, we have customers, we have inventory. ~~And also a lot more, of course, ~~when the business is growing.\\n\\nBut let's simplify to this. We have [00:02:00] one person that is working with this. We have another one that is working with this. We have a third one that is working with this and a fourth one. ~~Okay. ~~If there's a lot of people working with these Excel sheets and we get a lot of data, then it's quite easy to get duplicates, for example.\\n\\nCommon problems that ~~are ~~arose when sharing Excel sheets in this way, we get data duplication. Accidentally ~~we create ~~creating duplicates without knowing ~~about ~~that these lines may have existed elsewhere. ~~That is ~~and that is the problem that we need to manually update in several places.\\n\\nAnd we get inconsistent data. For example, we look for one person and then suddenly there is a similar person with different addresses or same person with different addresses. Then we have an inconsistency in addresses, for example. One is [00:03:00] a typo. Another one is not we have relationships.\\n\\nIt's hard to manage manually between customers and orders, which customer connects to which orders that is hard to manage manually. Team, they manually links the order to the right customer. This takes a lot of time and it's manual effort, and it will lead to inconsistencies and bugs like you, you do errors because humans it's human is not good at manual work performance.\\n\\nOur Excel sheet is growing larger and larger, and this will cause performance issues. It'll get slow ~~to ~~to work with huge amounts of data in Excel and many more problems that we won't go into in this slide here. The idea is that we need something else ~~to ~~to manage this when our company's [00:04:00] growing.\\n\\nAn example of inconsistent data. You can see it here due to manual input. For example, here you have Alice Frost, alice@example.com and this address, and then you have Alice f alice\\\\_f@example.com. Okay? It seems to be the same person or same phone number, same address. But here we have LSF, here we have Alice Frost.\\n\\nHere are Alice F at example. Here we have Alice at example. Is it the same person or not? That is a question. Is this the same person? This is and also we will come back more into how to handle inconsistent data and how to. Make sure that the data is unique and what is determining a unique row, et cetera.\\n\\nThis will come back more into a data modeling course later on. However, for ~~this in ~~this course, we'll focus a lot in sk l queries and how to analyze the data. ~~And ~~this is due [00:05:00] to being able to both learn. Fundamentals in SQL and to work with it with analytics and for transactional later on.\\n\\nAnd also we will learn it ~~for ~~that you can go~~ on ~~for example, ~~go ~~towards data warehouses and you do need to understand how to analyze the data. But that was a side note on what I just said. Now, which ~~Alice ~~is linked to what orders in the orders table. Using relational databases and SQL, we can handle many of these issues.\\n\\nSQL is a structured query language ~~and we work, ~~and the database is a relational database, there are also non relational databases such as a document database, vector database, et cetera. And no SQL. But here we have relational databases and SQL, they follow something called the relational model that will come back later on as [00:06:00] well.\\n\\nDefine relationships in SQL tables. ~~If ~~this ensures data consistency. ~~Op, ~~they're optimized for large volumes of data, scalable and efficient for querying of data. Data constraints, like data types and unique values ~~are ~~are put in ~~that they ~~make sure that the data conforms to a specific type.\\n\\nFor example, you have a numeric column, then you cannot put in strings or ~~other ~~other types of data. ~~And ~~and you have unique values. You can make unique constraints. This makes sure that we get automatic validation, which reduces error. I talk about this type of thing.\\n\\nThis part is more concern with something called OLTP that will come back to more transactional databases. And there's a lot of ~~more more ~~nice benefits ~~with ~~with a database. SKL, what is this? [00:07:00] What is SGL is structured query language. We have create, update, and we organize data into tables.\\n\\nOkay? Tables, we have rows and columns. You can see it is similar to the Excel sheets. You have rows and columns, right? And SGL has standardized language. It's standardized. There, there is a lot of different flavors of SQL, but mostly it's similar. And ~~there, ~~there's some syntax that differs.\\n\\nBut the foundation is the same. It's based on a relational model. And what this means is that we organize data into related tables. We link the data through unique identifiers. ~~What ~~if you don't understand what this means right now, it's fine. And we'll come back to this ~~term ~~terminology later on.\\n\\nThere's some core functions within SQL. [00:08:00] We get DDL data definition language.\\n\\n**Kokchun Giang-3:** define structure of database, for example tables and what type of data it holds. We use the crate alter and drop, for example. We can create table, we can alter the table, we can drop tables. This is DDL, data, definition language. Moving on, ~~we have let's see. Oh, sorry. ~~We have data manipulation language, DML, these are to manipulate data directly in the database.\\n\\nwe have insert command, we have update command, we have delete command,\\n\\nand then we have DQL data query language. It's to select to you, you use select statement to retrieve a specific information from a database. Letting users access and fill the data for insights. We use the select statement and or the select class.\\n\\nWe'll see. We will use it very a [00:09:00] lot in this course to analyze the data and we use it together with a lot of other things to filter. For example, the wear class, the group buy, et cetera.\\n\\nAnd then we have DCL data control language. We have here you can revoke and grant to give access to other uses.\\n\\nYou, you may want to give access to certain uses for certain type, certain database or certain tables, right? Depending on which SQL you use there's a possibility for more or less. DCL. Yes. And then we go into the SQL flavor that we will use in this course we'll use Duck db.\\n\\nMeet Duck db a modern, powerful database management system for analytics. It's super powerful, it's super performant. ~~And also it's something called oap. Opt, ~~it's optimized for intensive analytical [00:10:00] queries that you can do locally in your own computer. And it's very easy to set up. ~~It's it's just a, ~~you just use a ~~file, a duct ~~DB file, and then you can connect to it in the terminal.\\n\\nAnd also we talked about Ola. Then we need to mention OLTP, which is for transactional database. That is optimized for a lot of inserts into the database. For example, other ~~SGL other ~~RD. Such as Postgres, ~~SGL, ~~Microsoft ~~sgl ~~they use and they are ~~OLCP, ~~right?\\n\\nWhile ~~Dtb, RO ~~is Ola. It's very highly performant, on your own machine and can handle large data sets. It's embedded database, no need for separate server and database is contained in the file. This makes it very simple to work with and very good to start with as a as a first course within ~~SKL ~~analytics.\\n\\nTo just learn how to get insights from the data, [00:11:00] to filter the data, to manipulate it in the way that you want. And it integrates very well with other tools in the data science ecosystem or data engineering, data analytics ecosystem such as Python. Pandas and data frames. We will work with it together with pandas in this course that you can see the power of combining Python and ~~a scale combining Python and Duct ~~DB in order ~~to ~~to get data insights that you want.\\n\\nVery good for data analysis, can run complex queries for analytics and reporting. And it's very good for building data transformations in an ETL pipeline to serve business intelligence and ai. We'll come back ~~in ~~in more if you follow me, ~~we have a lot of, ~~I have courses in ~~data within ~~data engineering, a lot of them.\\n\\nAnd then there I usually build ~~pipelines ~~ETL or ELT pipelines to be more. Specific. ~~And ~~and there DDB is a core part. Could be a core [00:12:00] part of it where you store the data. You could use DDB as the data warehouse, for example. A data engineering pipeline with an OLA database as a data warehouse.\\n\\nThe reason why I picked data engineering pipeline is that when you're working with data analytics, you usually~~ you ~~have the data. You want the data to go through different types of transformations. Here's~~ engineering pipe, ~~data, engineering pipeline, and we use for example, DDB as the data warehouse.\\n\\nBut you could use something else, ~~a cloud, ~~such as snowflake, ~~for example for that but or ~~Amazon, Redshift ~~or some, ~~or Google BigQuery. But~~ but TDB ~~could work for local solution. What you have is that you have different data sources. ~~This ~~this could be like CSV files ~~here. It could have like ~~APIs.\\n\\nYou could have other types of data. And they are ingested into a data warehouse. The data warehouse could be this DDB file, for example. And then you serve dashboard. You [00:13:00] serve machine learning models. Then you need to transform the data ~~into certain way, ~~certain formats ~~that ~~they can be served ~~to this downstream ~~downstream.\\n\\nApplications. Here you have ELT. Extract and load. You extract and load the data into your data warehouse. Then you transform it inside your data warehouse, and then you serve the data for the end users. And the dashboards and the ML models. Here, for example, other people can take over.\\n\\nFor example, the BI analyst could take over here could be like a machine learning engineer or a data scientist that take over and~~ and and take ~~take the data from the data warehouse. Ddb, it could work as a lightweight data warehouse for small to medium sized. Data. ~~And if you ~~and if you grow out of your DDB database that it means that you cannot do it locally anymore, then you can consider ~~cloud ~~cloud solutions.\\n\\nBut they ~~cost the ~~cost money. Okay? Yes.\\n\\nthat was an [00:14:00] introduction to ~~sq, ~~SQL and DDB ~~in ~~and kind of introduction to this course as well. But we'll have more introductions such as the core structure that you can see what is the actual content of this course. And, who this is suitable for. ~~And here but ~~I hope that you have learned some basic concepts about the motivation, about ~~the ~~why to use SQL and ~~the ~~databases and then also going into ~~like ~~DDB and some motivation about that.\\n\\nStay attuned for more videos where we'll actually go into coding and more practical development and analyzing of data. And that will be really fun. I hope to see you there and thanks ~~to what ~~for watching this video and see you in the next one ~~by, I.~~\\n\\n,\\n        Content: https://www.youtube.com/@AIgineer,\\n    \",\n",
    "        \"tool_call_id\": \"pyd_ai_dc9a3e8c4ef84081a5b513a0e217df0a\",\n",
    "        \"metadata\": \"null\",\n",
    "        \"timestamp\": \"2025-12-10T15:20:44.964819+00:00\",\n",
    "        \"part_kind\": \"tool-return\"\n",
    "      }\n",
    "    ],\n",
    "    \"instructions\": \"null\",\n",
    "    \"kind\": \"request\",\n",
    "    \"run_id\": \"544ffc36-8efa-4867-af31-fc57b63cdc40\",\n",
    "    \"metadata\": \"null\"\n",
    "  },\n",
    "  {\n",
    "    \"parts\": [\n",
    "      {\n",
    "        \"tool_name\": \"final_result\",\n",
    "        \"args\": {\n",
    "          \"video_title\": \"sql analytics with duckdb - introduction\",\n",
    "          \"video_link\": \"https://www.youtube.com/@AIgineer\",\n",
    "          \"answer\": \"Ro Bt my friend! The video \\\"sql analytics with duckdb - introduction\\\" provides an excellent foundational understanding of SQL, particularly its application in analytics with DuckDB. It effectively illustrates the challenges of managing growing datasets with tools like Excel sheets, highlighting issues such as data duplication and inconsistency. SQL, or Structured Query Language, is presented as the robust solution, leveraging relational databases to standardize data definition, manipulation, and querying. The video breaks down SQL's core components: DDL (Data Definition Language) for defining database structures, DML (Data Manipulation Language) for modifying data, DQL (Data Query Language) for retrieving information (the beloved SELECT statement!), and DCL (Data Control Language) for managing access. From an AI/Data Engineering perspective, understanding SQL is paramount for building robust data pipelines and performing effective data analysis. The video introduces DuckDB as a modern, high-performance analytical database, optimized for intensive analytical queries directly on your machine. Its embedded nature, ease of setup, and seamless integration with the Python data science ecosystem (like Pandas) make it an incredibly powerful tool for data engineers and analysts alike. DuckDB can even act as a lightweight data warehouse in ETL/ELT pipelines, empowering you to transform raw data into valuable insights for business intelligence and AI applications. It's a fantastic stepping stone for anyone looking to master data analytics and engineering locally before scaling to cloud solutions!\"\n",
    "        },\n",
    "        \"tool_call_id\": \"pyd_ai_21034c20ab84417daf087fdf5b1b9298\",\n",
    "        \"id\": \"null\",\n",
    "        \"provider_details\": {\n",
    "          \"thought_signature\": \"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\"\n",
    "        },\n",
    "        \"part_kind\": \"tool-call\"\n",
    "      }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "      \"input_tokens\": 3352,\n",
    "      \"cache_write_tokens\": 0,\n",
    "      \"cache_read_tokens\": 0,\n",
    "      \"output_tokens\": 929,\n",
    "      \"input_audio_tokens\": 0,\n",
    "      \"cache_audio_read_tokens\": 0,\n",
    "      \"output_audio_tokens\": 0,\n",
    "      \"details\": {\n",
    "        \"thoughts_tokens\": 602,\n",
    "        \"text_prompt_tokens\": 3352\n",
    "      }\n",
    "    },\n",
    "    \"model_name\": \"gemini-2.5-flash\",\n",
    "    \"timestamp\": \"2025-12-10T15:20:51.822031+00:00\",\n",
    "    \"kind\": \"response\",\n",
    "    \"provider_name\": \"google-gla\",\n",
    "    \"provider_details\": {\n",
    "      \"finish_reason\": \"STOP\"\n",
    "    },\n",
    "    \"provider_response_id\": \"0485afqGJb7txN8PjLDX2Qk\",\n",
    "    \"finish_reason\": \"stop\",\n",
    "    \"run_id\": \"544ffc36-8efa-4867-af31-fc57b63cdc40\",\n",
    "    \"metadata\": \"null\"\n",
    "  },\n",
    "  {\n",
    "    \"parts\": [\n",
    "      {\n",
    "        \"tool_name\": \"final_result\",\n",
    "        \"content\": \"Final result processed.\",\n",
    "        \"tool_call_id\": \"pyd_ai_21034c20ab84417daf087fdf5b1b9298\",\n",
    "        \"metadata\": \"null\",\n",
    "        \"timestamp\": \"2025-12-10T15:20:51.823306+00:00\",\n",
    "        \"part_kind\": \"tool-return\"\n",
    "      }\n",
    "    ],\n",
    "    \"instructions\": \"null\",\n",
    "    \"kind\": \"request\",\n",
    "    \"run_id\": \"544ffc36-8efa-4867-af31-fc57b63cdc40\",\n",
    "    \"metadata\": \"null\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3d21dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'User', 'content': 'video sql'}]\n"
     ]
    }
   ],
   "source": [
    "readable_history = []\n",
    "for turn in reply:\n",
    "    for part in turn[\"parts\"]:\n",
    "        if \"content\" in part and \"part_kind\" in part:\n",
    "            part_kind = part[\"part_kind\"]\n",
    "            content = part[\"content\"]\n",
    "            if part_kind == \"user-prompt\":\n",
    "                role = \"User\"\n",
    "                #print(\"user-prompt\")\n",
    "                #print(content)\n",
    "            elif part_kind == 'tool-return':\n",
    "                continue\n",
    "            elif part_kind == 'tool-call':\n",
    "                continue\n",
    "            elif part_kind == \"assistant-response\":\n",
    "                role = \"Bot\"\n",
    "            else:\n",
    "                continue\n",
    "            readable_history.append({\n",
    "                    \"role\": role,\n",
    "                    \"content\": content\n",
    "                    })\n",
    "print(readable_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-lab-de24-john-sandsjo (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
